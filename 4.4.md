# 4.4 深入瞭解 ChatModel

> **本章重點**：深入探討 Spring AI 的核心架構，理解 ChatModel 與 ChatClient 的關係，掌握不同 AI 模型的特性和企業級應用策略。

## 🎯 學習目標

完成本章學習後，您將能夠：

- 🎯 **理解 Spring AI 架構**：掌握 ChatModel 和 ChatClient 的完整架構關係
- 🎯 **選擇合適的 AI 模型**：了解不同模型的特性和適用場景
- 🎯 **配置模型參數**：學會調整溫度、token 限制等關鍵參數
- 🎯 **實現模型切換**：建立彈性的多模型支援架構
- 🎯 **優化模型效能**：掌握快取、連線池等效能優化技術

---

## 4.4.1 Spring AI 架構全貌

### ChatModel vs ChatClient 關係圖

![ChatModel 架構](https://ithelp.ithome.com.tw/upload/images/20240810/20161290nfq1H0dDoW.jpg)

到目前為止我們做得比直接上 ChatGPT 發問還不如，ChatGPT 起碼還能記住你是誰（不信自己去問），今天我們就來深入了解 Spring AI 1.0 GA 中 ChatClient 和 ChatModel 的完整架構與功能。

```
┌─────────────────────────────────────────────────┐
│                  ChatClient                     │
│  ┌─────────────────────────────────────────┐    │
│  │            Fluent API Layer             │    │
│  │  • prompt()                             │    │
│  │  • system() / user()                    │    │
│  │  • call() / stream()                    │    │
│  │  • content() / chatResponse()           │    │
│  └─────────────────────────────────────────┘    │
│                     │                           │
│                     ▼                           │
│  ┌─────────────────────────────────────────┐    │
│  │              ChatModel                  │    │
│  │  ┌─────────────────────────────────┐    │    │
│  │  │        Model Abstraction        │    │    │
│  │  │  • call(String)                 │    │    │
│  │  │  • call(Prompt)                 │    │    │
│  │  │  • stream(Prompt)               │    │    │
│  │  └─────────────────────────────────┘    │    │
│  └─────────────────────────────────────────┘    │
│                     │                           │
│                     ▼                           │
│  ┌─────────────────────────────────────────┐    │
│  │           Provider Implementations      │    │
│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐    │    │
│  │  │ OpenAI  │ │  Azure  │ │  Groq   │    │    │
│  │  │ ChatModel│ │ChatModel│ │ChatModel│    │    │
│  │  └─────────┘ └─────────┘ └─────────┘    │    │
│  └─────────────────────────────────────────┘    │
└─────────────────────────────────────────────────┘
```

### 架構層級說明

**1. ChatClient（高級 API 層）**
- 提供現代化的 Fluent API 介面
- 類似於 Spring 生態系統中的 RestClient、WebClient
- 內建常用功能：系統提示詞、流式輸出、錯誤處理
- 推薦用於大部分應用場景

**2. ChatModel（抽象層）**
- 定義與 AI 模型互動的核心介面
- 提供統一的 API 抽象，隱藏不同提供商的差異
- 支援同步和異步（流式）呼叫
- 適合需要底層控制的場景

**3. Provider Implementations（實現層）**
- 各 AI 服務提供商的具體實現
- 處理特定提供商的 API 協定和認證
- 負責請求轉換和回應解析

---

## 4.4.2 ChatModel 核心介面

### 基本介面定義

```java
public interface ChatModel extends Model<Prompt, ChatResponse>, StreamingChatModel {
    
    /**
     * 簡單字串呼叫
     * @param message 使用者訊息
     * @return AI 回應內容
     */
    default String call(String message) {
        Prompt prompt = new Prompt(new UserMessage(message));
        return call(prompt).getResult().getOutput().getContent();
    }
    
    /**
     * 完整 Prompt 呼叫
     * @param prompt 完整的提示詞物件
     * @return 完整的聊天回應
     */
    @Override
    ChatResponse call(Prompt prompt);
}

public interface StreamingChatModel extends StreamingModel<Prompt, ChatResponse> {
    
    /**
     * 流式呼叫
     * @param prompt 提示詞物件
     * @return 流式聊天回應
     */
    @Override
    Flux<ChatResponse> stream(Prompt prompt);
    
    /**
     * 簡單流式呼叫
     * @param message 使用者訊息
     * @return 流式回應內容
     */
    default Flux<String> stream(String message) {
        return stream(new Prompt(new UserMessage(message)))
                .map(response -> response.getResult().getOutput().getContent());
    }
}
```

### 實際使用範例

```java
package com.example.service;

import lombok.RequiredArgsConstructor;
import lombok.extern.slf4j.Slf4j;
import org.springframework.ai.chat.model.ChatModel;
import org.springframework.ai.chat.model.StreamingChatModel;
import org.springframework.ai.chat.prompt.*;
import org.springframework.stereotype.Service;
import reactor.core.publisher.Flux;

import java.util.List;
import java.util.Map;

@Service
@RequiredArgsConstructor
@Slf4j
public class ChatModelService {
    
    private final ChatModel chatModel;
    private final StreamingChatModel streamingChatModel;
    
    /**
     * 簡單字串呼叫
     */
    public String simpleCall(String message) {
        return chatModel.call(message);
    }
    
    /**
     * 使用 Prompt 物件的完整呼叫
     */
    public ChatResponse fullCall(String systemMessage, String userMessage) {
        Prompt prompt = new Prompt(List.of(
            new SystemMessage(systemMessage),
            new UserMessage(userMessage)
        ));
        
        return chatModel.call(prompt);
    }
    
    /**
     * 使用 PromptTemplate 的動態呼叫
     */
    public String templateCall(String template, Map<String, Object> variables) {
        PromptTemplate promptTemplate = new PromptTemplate(template);
        Prompt prompt = promptTemplate.create(variables);
        
        ChatResponse response = chatModel.call(prompt);
        return response.getResult().getOutput().getContent();
    }
    
    /**
     * 流式呼叫
     */
    public Flux<String> streamCall(String message) {
        return streamingChatModel.stream(message);
    }
    
    /**
     * 帶選項的流式呼叫
     */
    public Flux<String> streamCallWithOptions(String message, ChatOptions options) {
        Prompt prompt = new Prompt(new UserMessage(message), options);
        return streamingChatModel.stream(prompt)
                .map(response -> response.getResult().getOutput().getContent());
    }
}
```

---

## 4.4.3 AI 模型參數配置

### 核心參數說明

```java
package com.example.config;

import org.springframework.ai.openai.OpenAiChatOptions;
import org.springframework.ai.openai.api.OpenAiApi;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;

@Configuration
public class ChatModelConfig {
    
    /**
     * 預設聊天選項配置
     */
    @Bean
    public OpenAiChatOptions defaultChatOptions() {
        return OpenAiChatOptions.builder()
                .model("gpt-4o-mini")        // 模型選擇
                .temperature(0.7F)           // 創意度：0.0-2.0
                .maxTokens(1000)             // 最大輸出 token 數
                .topP(1.0F)                  // 核心採樣：0.0-1.0
                .frequencyPenalty(0.0F)      // 頻率懲罰：-2.0-2.0
                .presencePenalty(0.0F)       // 存在懲罰：-2.0-2.0
                .build();
    }
    
    /**
     * 創意寫作專用配置
     */
    @Bean("creativeChatOptions")
    public OpenAiChatOptions creativeChatOptions() {
        return OpenAiChatOptions.builder()
                .model("gpt-4o")
                .temperature(1.2F)           // 高創意度
                .maxTokens(2000)             // 較長輸出
                .topP(0.9F)                  // 稍微限制選擇範圍
                .frequencyPenalty(0.5F)      // 避免重複
                .presencePenalty(0.3F)       // 鼓勵新話題
                .build();
    }
    
    /**
     * 程式碼生成專用配置
     */
    @Bean("codeChatOptions")
    public OpenAiChatOptions codeChatOptions() {
        return OpenAiChatOptions.builder()
                .model("gpt-4o")
                .temperature(0.2F)           // 低創意度，更精確
                .maxTokens(1500)             // 適中輸出長度
                .topP(0.95F)                 // 較保守的選擇
                .frequencyPenalty(0.0F)      // 不懲罰重複（程式碼可能需要重複結構）
                .presencePenalty(0.0F)
                .build();
    }
}
```

### 參數詳細說明

| 參數 | 範圍 | 說明 | 使用建議 |
|------|------|------|----------|
| **Temperature** | 0.0-2.0 | 控制回應的隨機性和創意度 | 0.2（精確）、0.7（平衡）、1.2（創意） |
| **Max Tokens** | 1-4096+ | 限制回應的最大長度 | 根據需求設定，注意成本控制 |
| **Top P** | 0.0-1.0 | 核心採樣，控制詞彙選擇範圍 | 0.9-1.0 較常用 |
| **Frequency Penalty** | -2.0-2.0 | 懲罰重複出現的詞彙 | 0.0-0.5 避免過度重複 |
| **Presence Penalty** | -2.0-2.0 | 鼓勵談論新話題 | 0.0-0.3 增加話題多樣性 |

### 動態參數調整

```java
@Service
public class AdaptiveChatService {
    
    private final ChatModel chatModel;
    
    /**
     * 根據內容類型動態調整參數
     */
    public String adaptiveChat(String message, ContentType contentType) {
        OpenAiChatOptions options = switch (contentType) {
            case CREATIVE_WRITING -> OpenAiChatOptions.builder()
                .temperature(1.2F)
                .maxTokens(2000)
                .build();
                
            case CODE_GENERATION -> OpenAiChatOptions.builder()
                .temperature(0.2F)
                .maxTokens(1500)
                .build();
                
            case TECHNICAL_EXPLANATION -> OpenAiChatOptions.builder()
                .temperature(0.5F)
                .maxTokens(1200)
                .build();
                
            case CASUAL_CHAT -> OpenAiChatOptions.builder()
                .temperature(0.8F)
                .maxTokens(800)
                .build();
        };
        
        Prompt prompt = new Prompt(new UserMessage(message), options);
        return chatModel.call(prompt).getResult().getOutput().getContent();
    }
    
    public enum ContentType {
        CREATIVE_WRITING, CODE_GENERATION, TECHNICAL_EXPLANATION, CASUAL_CHAT
    }
}
```

---

## 4.4.4 多模型支援架構

### 模型選擇策略

```java
package com.example.config;

import org.springframework.ai.chat.model.ChatModel;
import org.springframework.ai.openai.OpenAiChatModel;
import org.springframework.ai.openai.api.OpenAiApi;
import org.springframework.beans.factory.annotation.Qualifier;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.context.annotation.Bean;
import org.springframework.context.annotation.Configuration;
import org.springframework.context.annotation.Primary;

@Configuration
public class MultiModelConfig {
    
    @Value("${spring.ai.openai.api-key}")
    private String openaiApiKey;
    
    @Value("${spring.ai.groq.api-key:}")
    private String groqApiKey;
    
    /**
     * 高性能模型 - GPT-4o
     */
    @Bean("highPerformanceModel")
    public ChatModel highPerformanceModel() {
        var openAiApi = new OpenAiApi(openaiApiKey);
        var options = OpenAiChatOptions.builder()
            .model("gpt-4o")
            .temperature(0.7F)
            .maxTokens(2000)
            .build();
        return new OpenAiChatModel(openAiApi, options);
    }
    
    /**
     * 經濟型模型 - GPT-4o mini
     */
    @Bean("economicModel")
    @Primary
    public ChatModel economicModel() {
        var openAiApi = new OpenAiApi(openaiApiKey);
        var options = OpenAiChatOptions.builder()
            .model("gpt-4o-mini")
            .temperature(0.7F)
            .maxTokens(1000)
            .build();
        return new OpenAiChatModel(openAiApi, options);
    }
    
    /**
     * 高速模型 - Groq
     */
    @Bean("speedModel")
    public ChatModel speedModel() {
        if (groqApiKey == null || groqApiKey.isEmpty()) {
            return economicModel(); // 降級到經濟型模型
        }
        
        var groqApi = new OpenAiApi("https://api.groq.com/openai/v1", groqApiKey);
        var options = OpenAiChatOptions.builder()
            .model("llama-3.1-70b-versatile")
            .temperature(0.7F)
            .maxTokens(1000)
            .build();
        return new OpenAiChatModel(groqApi, options);
    }
}
```

### 智能模型路由

```java
@Service
public class ModelRouterService {
    
    private final ChatModel highPerformanceModel;
    private final ChatModel economicModel;
    private final ChatModel speedModel;
    
    public ModelRouterService(
            @Qualifier("highPerformanceModel") ChatModel highPerformanceModel,
            @Qualifier("economicModel") ChatModel economicModel,
            @Qualifier("speedModel") ChatModel speedModel) {
        this.highPerformanceModel = highPerformanceModel;
        this.economicModel = economicModel;
        this.speedModel = speedModel;
    }
    
    /**
     * 根據任務複雜度選擇模型
     */
    public String routeByComplexity(String message, TaskComplexity complexity) {
        ChatModel selectedModel = switch (complexity) {
            case HIGH -> {
                log.info("使用高性能模型處理複雜任務");
                yield highPerformanceModel;
            }
            case MEDIUM -> {
                log.info("使用經濟型模型處理中等任務");
                yield economicModel;
            }
            case LOW -> {
                log.info("使用高速模型處理簡單任務");
                yield speedModel;
            }
        };
        
        return selectedModel.call(message);
    }
    
    /**
     * 根據使用者等級選擇模型
     */
    public String routeByUserTier(String message, UserTier userTier) {
        ChatModel selectedModel = switch (userTier) {
            case PREMIUM -> highPerformanceModel;
            case STANDARD -> economicModel;
            case FREE -> speedModel;
        };
        
        return selectedModel.call(message);
    }
    
    /**
     * 容錯路由 - 自動降級
     */
    public String routeWithFallback(String message) {
        List<ChatModel> models = List.of(
            highPerformanceModel,
            economicModel,
            speedModel
        );
        
        for (ChatModel model : models) {
            try {
                return model.call(message);
            } catch (Exception e) {
                log.warn("模型呼叫失敗，嘗試下一個模型：{}", e.getMessage());
            }
        }
        
        throw new RuntimeException("所有 AI 模型都不可用");
    }
    
    public enum TaskComplexity {
        HIGH, MEDIUM, LOW
    }
    
    public enum UserTier {
        PREMIUM, STANDARD, FREE
    }
}
```

---

## 4.4.5 效能優化與快取

### 回應快取機制

```java
package com.example.service;

import org.springframework.cache.annotation.Cacheable;
import org.springframework.stereotype.Service;
import java.security.MessageDigest;
import java.nio.charset.StandardCharsets;

@Service
public class CachedChatService {
    
    private final ChatModel chatModel;
    
    /**
     * 快取常見問題的回應
     */
    @Cacheable(value = "chatResponses", key = "#root.methodName + '_' + T(java.util.Objects).hash(#message)")
    public String getCachedResponse(String message) {
        log.info("快取未命中，呼叫 AI 模型：{}", message);
        return chatModel.call(message);
    }
    
    /**
     * 根據內容雜湊快取
     */
    @Cacheable(value = "chatResponses", key = "#root.target.generateCacheKey(#message, #systemPrompt)")
    public String getCachedResponseWithSystem(String message, String systemPrompt) {
        Prompt prompt = new Prompt(List.of(
            new SystemMessage(systemPrompt),
            new UserMessage(message)
        ));
        
        return chatModel.call(prompt).getResult().getOutput().getContent();
    }
    
    /**
     * 生成快取鍵
     */
    public String generateCacheKey(String message, String systemPrompt) {
        try {
            String combined = (systemPrompt != null ? systemPrompt : "") + "|" + message;
            MessageDigest digest = MessageDigest.getInstance("SHA-256");
            byte[] hash = digest.digest(combined.getBytes(StandardCharsets.UTF_8));
            return bytesToHex(hash).substring(0, 16); // 取前16字元
        } catch (Exception e) {
            return String.valueOf(Objects.hash(message, systemPrompt));
        }
    }
    
    private String bytesToHex(byte[] bytes) {
        StringBuilder result = new StringBuilder();
        for (byte b : bytes) {
            result.append(String.format("%02x", b));
        }
        return result.toString();
    }
}
```

### 快取配置

```java
@Configuration
@EnableCaching
public class CacheConfig {
    
    @Bean
    public CacheManager cacheManager() {
        CaffeineCacheManager cacheManager = new CaffeineCacheManager();
        cacheManager.setCaffeine(Caffeine.newBuilder()
            .maximumSize(1000)  // 最大快取條目數
            .expireAfterWrite(Duration.ofHours(1))  // 1小時後過期
            .recordStats());  // 啟用統計
        return cacheManager;
    }
}
```

### 連線池優化

```yaml
# application.yml
spring:
  ai:
    openai:
      api-key: ${OPENAI_API_KEY}
      base-url: https://api.openai.com
      chat:
        options:
          model: gpt-4o-mini
          temperature: 0.7
      # 連線池配置
      client:
        connect-timeout: 10s
        read-timeout: 60s
        max-connections: 20
        max-connections-per-route: 10

# 快取配置
cache:
  caffeine:
    spec: maximumSize=1000,expireAfterWrite=1h
```

---

## 4.4.6 監控與診斷

### 模型使用統計

```java
@Component
public class ChatModelMetrics {
    
    private final MeterRegistry meterRegistry;
    private final Counter requestCounter;
    private final Timer responseTimer;
    private final Gauge activeConnections;
    
    public ChatModelMetrics(MeterRegistry meterRegistry) {
        this.meterRegistry = meterRegistry;
        this.requestCounter = Counter.builder("ai.chat.requests")
            .description("AI chat requests count")
            .register(meterRegistry);
        this.responseTimer = Timer.builder("ai.chat.response.time")
            .description("AI chat response time")
            .register(meterRegistry);
        this.activeConnections = Gauge.builder("ai.chat.connections.active")
            .description("Active AI chat connections")
            .register(meterRegistry, this, ChatModelMetrics::getActiveConnections);
    }
    
    public void recordRequest(String model, Duration responseTime, boolean success) {
        requestCounter.increment(
            Tags.of(
                "model", model,
                "status", success ? "success" : "error"
            )
        );
        responseTimer.record(responseTime, Tags.of("model", model));
    }
    
    private double getActiveConnections() {
        // 實際實現需要根據連線池狀態
        return 0.0;
    }
}
```

### 健康檢查

```java
@Component
public class ChatModelHealthIndicator implements HealthIndicator {
    
    private final ChatModel chatModel;
    
    @Override
    public Health health() {
        try {
            // 簡單的健康檢查
            String response = chatModel.call("Hello");
            
            if (response != null && !response.isEmpty()) {
                return Health.up()
                    .withDetail("model", "available")
                    .withDetail("response_length", response.length())
                    .build();
            } else {
                return Health.down()
                    .withDetail("model", "no_response")
                    .build();
            }
        } catch (Exception e) {
            return Health.down()
                .withDetail("model", "error")
                .withDetail("error", e.getMessage())
                .build();
        }
    }
}
```

---

## 4.4.7 實際應用範例

### 企業級聊天服務

```java
@Service
@Transactional
public class EnterpriseChatService {
    
    private final ModelRouterService modelRouter;
    private final CachedChatService cachedChat;
    private final ChatModelMetrics metrics;
    
    /**
     * 企業級聊天處理
     */
    public ChatResult processChat(ChatRequest request) {
        Instant startTime = Instant.now();
        String model = "unknown";
        boolean success = false;
        
        try {
            // 1. 內容安全檢查
            if (!isContentSafe(request.getMessage())) {
                throw new ContentSafetyException("內容包含不當資訊");
            }
            
            // 2. 使用者權限檢查
            UserTier userTier = getUserTier(request.getUserId());
            
            // 3. 選擇合適的模型
            TaskComplexity complexity = analyzeComplexity(request.getMessage());
            
            // 4. 嘗試快取
            String cacheKey = generateCacheKey(request);
            String cachedResponse = getCachedResponse(cacheKey);
            if (cachedResponse != null) {
                return ChatResult.fromCache(cachedResponse);
            }
            
            // 5. 呼叫 AI 模型
            String response = modelRouter.routeByComplexity(
                request.getMessage(), complexity
            );
            
            // 6. 儲存快取
            saveCachedResponse(cacheKey, response);
            
            // 7. 記錄使用統計
            recordUsage(request.getUserId(), model, response.length());
            
            success = true;
            return ChatResult.success(response, model);
            
        } catch (Exception e) {
            log.error("聊天處理失敗", e);
            return ChatResult.error(e.getMessage());
        } finally {
            Duration responseTime = Duration.between(startTime, Instant.now());
            metrics.recordRequest(model, responseTime, success);
        }
    }
    
    private boolean isContentSafe(String message) {
        // 實現內容安全檢查邏輯
        return !message.toLowerCase().contains("不當內容");
    }
    
    private TaskComplexity analyzeComplexity(String message) {
        // 簡單的複雜度分析
        if (message.length() > 500 || message.contains("複雜") || message.contains("詳細")) {
            return TaskComplexity.HIGH;
        } else if (message.length() > 100) {
            return TaskComplexity.MEDIUM;
        } else {
            return TaskComplexity.LOW;
        }
    }
}

@Data
public class ChatResult {
    private String response;
    private String model;
    private boolean fromCache;
    private boolean success;
    private String error;
    
    public static ChatResult success(String response, String model) {
        ChatResult result = new ChatResult();
        result.setResponse(response);
        result.setModel(model);
        result.setSuccess(true);
        result.setFromCache(false);
        return result;
    }
    
    public static ChatResult fromCache(String response) {
        ChatResult result = new ChatResult();
        result.setResponse(response);
        result.setSuccess(true);
        result.setFromCache(true);
        return result;
    }
    
    public static ChatResult error(String error) {
        ChatResult result = new ChatResult();
        result.setError(error);
        result.setSuccess(false);
        return result;
    }
}
```

---

## 📝 本章重點回顧

1. **Spring AI 架構**：深入理解了 ChatClient 和 ChatModel 的分層架構
2. **模型參數配置**：掌握了溫度、token 限制等關鍵參數的調整
3. **多模型支援**：學會了建立彈性的模型選擇和路由機制
4. **效能優化**：了解了快取、連線池等效能優化技術
5. **企業級應用**：建立了完整的企業級聊天服務架構

### ChatClient vs ChatModel 選擇指南

| 場景 | 推薦選擇 | 理由 |
|------|----------|------|
| **一般應用開發** | ChatClient | Fluent API，易用性高 |
| **複雜提示詞組合** | ChatClient | 內建系統提示詞支援 |
| **流式輸出** | ChatClient | 更簡潔的流式 API |
| **底層控制** | ChatModel | 直接存取 Prompt 物件 |
| **自定義模板** | ChatModel | 完整的 PromptTemplate 支援 |
| **效能關鍵場景** | ChatModel | 更少的抽象層級 |

### 下一步學習方向

在下一章中，我們將學習前端如何處理流式輸出，建立完整的前後端整合方案，為使用者提供最佳的 AI 互動體驗。

---

**參考資料：**
- [Spring AI ChatModel Documentation](https://docs.spring.io/spring-ai/reference/api/chatmodel.html)
- [OpenAI API Parameters](https://platform.openai.com/docs/api-reference/chat/create)
- [Spring Boot Caching](https://docs.spring.io/spring-boot/docs/current/reference/html/io.html#io.caching)
- [Micrometer Metrics](https://micrometer.io/docs)