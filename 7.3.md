# 7.3 ETL (ä¸Š) - RAG çš„çŸ¥è­˜ä¾†æº

> **æœ¬ç« é‡é»**ï¼šæ·±å…¥æ¢è¨ RAG ç³»çµ±çš„å„ç¨®çŸ¥è­˜ä¾†æºï¼ŒæŒæ¡ ETL æµç¨‹ä¸­çš„è³‡æ–™æå–ï¼ˆExtractï¼‰æŠ€è¡“ï¼Œå­¸æœƒæ•´åˆå¤šå…ƒåŒ–çš„ä¼æ¥­è³‡æ–™ä¾†æºã€‚

## ğŸ¯ å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬ç« å­¸ç¿’å¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š

- ğŸ¯ **ç†è§£ ETL åŸºç¤æ¦‚å¿µ**ï¼šæŒæ¡ Extractã€Transformã€Load çš„å®Œæ•´æµç¨‹
- ğŸ¯ **è­˜åˆ¥çŸ¥è­˜ä¾†æºé¡å‹**ï¼šäº†è§£ä¼æ¥­ä¸­å„ç¨®è³‡æ–™ä¾†æºçš„ç‰¹é»å’ŒæŒ‘æˆ°
- ğŸ¯ **å¯¦ç¾è³‡æ–™æå–å™¨**ï¼šé–‹ç™¼å¤šç¨®è³‡æ–™ä¾†æºçš„æå–å™¨
- ğŸ¯ **è¨­è¨ˆ ETL æ¶æ§‹**ï¼šå»ºç«‹å¯æ“´å±•çš„ ETL ç³»çµ±æ¶æ§‹
- ğŸ¯ **è™•ç†è³‡æ–™å“è³ª**ï¼šæŒæ¡è³‡æ–™æ¸…ç†å’Œå“è³ªæ§åˆ¶æŠ€è¡“

---

## 7.3.1 ETL åŸºç¤æ¦‚å¿µ

### ä»€éº¼æ˜¯ ETLï¼Ÿ

**ETLï¼ˆExtract, Transform, Loadï¼‰** æ˜¯è³‡æ–™è™•ç†çš„æ ¸å¿ƒæµç¨‹ï¼Œåœ¨ RAG ç³»çµ±ä¸­æ‰®æ¼”è‘—å°‡åŸå§‹è³‡æ–™è½‰æ›ç‚ºå¯æœå°‹çŸ¥è­˜çš„é—œéµè§’è‰²ã€‚

**ETL ä¸‰éšæ®µ**ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ETL Process Flow                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Extract (æå–)                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  â€¢ å¾å„ç¨®è³‡æ–™ä¾†æºæå–åŸå§‹è³‡æ–™                           â”‚ â”‚
â”‚  â”‚  â€¢ è™•ç†ä¸åŒæ ¼å¼å’Œå”è­°                                   â”‚ â”‚
â”‚  â”‚  â€¢ ç¢ºä¿è³‡æ–™å®Œæ•´æ€§å’Œä¸€è‡´æ€§                               â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â†“                                â”‚
â”‚  Transform (è½‰æ›)                                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  â€¢ æ¸…ç†å’Œæ¨™æº–åŒ–è³‡æ–™                                     â”‚ â”‚
â”‚  â”‚  â€¢ æ–‡æœ¬åˆ†å¡Šå’Œé è™•ç†                                     â”‚ â”‚
â”‚  â”‚  â€¢ å…ƒè³‡æ–™å¢å¼·å’Œæ¨™è¨˜                                     â”‚ â”‚
â”‚  â”‚  â€¢ å‘é‡åŒ–è™•ç†                                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                            â†“                                â”‚
â”‚  Load (è¼‰å…¥)                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  â€¢ å­˜å„²åˆ°å‘é‡è³‡æ–™åº«                                     â”‚ â”‚
â”‚  â”‚  â€¢ å»ºç«‹ç´¢å¼•å’Œé—œè¯                                       â”‚ â”‚
â”‚  â”‚  â€¢ æ›´æ–°çŸ¥è­˜åº«                                           â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### RAG ä¸­çš„ ETL ç‰¹é»

**å‚³çµ± ETL vs RAG ETL**ï¼š

| ç‰¹æ€§ | å‚³çµ± ETL | RAG ETL |
|------|----------|----------|
| **è³‡æ–™é¡å‹** | çµæ§‹åŒ–è³‡æ–™ | éçµæ§‹åŒ–æ–‡æœ¬ |
| **è™•ç†é‡é»** | è³‡æ–™æ¸…ç†ã€æ ¼å¼è½‰æ› | æ–‡æœ¬ç†è§£ã€èªç¾©æå– |
| **è¼¸å‡ºæ ¼å¼** | é—œè¯å¼è¡¨æ ¼ | å‘é‡åµŒå…¥ |
| **æ›´æ–°é »ç‡** | æ‰¹æ¬¡è™•ç† | å³æ™‚/è¿‘å³æ™‚ |
| **å“è³ªæŒ‡æ¨™** | å®Œæ•´æ€§ã€ä¸€è‡´æ€§ | èªç¾©æº–ç¢ºæ€§ã€ç›¸é—œæ€§ |

---

## 7.3.2 ä¼æ¥­çŸ¥è­˜ä¾†æºåˆ†é¡

### æ–‡æª”é¡è³‡æ–™ä¾†æº

**1. è¾¦å…¬æ–‡æª”**
```
â€¢ Microsoft Office æ–‡ä»¶
  - Word (.docx, .doc)
  - Excel (.xlsx, .xls)
  - PowerPoint (.pptx, .ppt)
  
â€¢ PDF æ–‡æª”
  - æ–‡æœ¬å‹ PDF
  - æƒæå‹ PDF (éœ€ OCR)
  - å—ä¿è­· PDF
  
â€¢ ç´”æ–‡æœ¬æ–‡ä»¶
  - TXT, RTF
  - Markdown (.md)
  - CSV, TSV
```

**2. æŠ€è¡“æ–‡æª”**
```
â€¢ ç¨‹å¼ç¢¼æ–‡æª”
  - README.md
  - API æ–‡æª”
  - ç¨‹å¼ç¢¼è¨»è§£
  
â€¢ è¨­è¨ˆæ–‡æª”
  - ç³»çµ±æ¶æ§‹åœ–
  - æµç¨‹åœ–
  - UML åœ–è¡¨
```

### ç¶²è·¯è³‡æº

**1. ä¼æ¥­ç¶²ç«™**
```
â€¢ å®˜æ–¹ç¶²ç«™å…§å®¹
â€¢ ç”¢å“èªªæ˜é é¢
â€¢ å¸¸è¦‹å•é¡Œ (FAQ)
â€¢ éƒ¨è½æ ¼æ–‡ç« 
â€¢ æ–°èç™¼å¸ƒ
```

**2. å…§éƒ¨ç³»çµ±**
```
â€¢ Wiki ç³»çµ±
â€¢ çŸ¥è­˜åº«å¹³å°
â€¢ å…§éƒ¨è«–å£‡
â€¢ å°ˆæ¡ˆç®¡ç†ç³»çµ±
```

### è³‡æ–™åº«è³‡æº

**1. é—œè¯å¼è³‡æ–™åº«**
```
â€¢ å®¢æˆ¶è³‡æ–™
â€¢ ç”¢å“è³‡è¨Š
â€¢ äº¤æ˜“è¨˜éŒ„
â€¢ æ—¥èªŒè³‡æ–™
```

**2. NoSQL è³‡æ–™åº«**
```
â€¢ MongoDB æ–‡æª”
â€¢ Elasticsearch ç´¢å¼•
â€¢ Redis å¿«å–è³‡æ–™
```

### API å’Œæœå‹™

**1. REST API**
```
â€¢ ç¬¬ä¸‰æ–¹æœå‹™ API
â€¢ å…§éƒ¨å¾®æœå‹™
â€¢ é›²ç«¯æœå‹™ API
```

**2. å³æ™‚è³‡æ–™æµ**
```
â€¢ Kafka è¨Šæ¯
â€¢ WebSocket è³‡æ–™
â€¢ äº‹ä»¶æµ
```

---

## 7.3.3 è³‡æ–™æå–å™¨æ¶æ§‹è¨­è¨ˆ

### Spring AI ETL æ¶æ§‹è¨­è¨ˆ

æ ¹æ“š Spring AI å®˜æ–¹æ–‡æª”ï¼ŒETL æ¶æ§‹åŒ…å«ä¸‰å€‹æ ¸å¿ƒä»‹é¢ï¼š

```java
/**
 * æ–‡æª”è®€å–å™¨ - æä¾›æ–‡æª”ä¾†æº
 */
public interface DocumentReader extends Supplier<List<Document>> {
    default List<Document> read() {
        return get();
    }
}

/**
 * æ–‡æª”è½‰æ›å™¨ - è™•ç†æ–‡æª”è½‰æ›
 */
public interface DocumentTransformer extends Function<List<Document>, List<Document>> {
    default List<Document> transform(List<Document> documents) {
        return apply(documents);
    }
}

/**
 * æ–‡æª”å¯«å…¥å™¨ - ç®¡ç†æœ€çµ‚å„²å­˜
 */
public interface DocumentWriter extends Consumer<List<Document>> {
    default void write(List<Document> documents) {
        accept(documents);
    }
}
```

### Spring AI Document æ¨¡å‹

```java
/**
 * Spring AI æ–‡æª”æ¨¡å‹
 */
public class Document {
    private String content;
    private Map<String, Object> metadata;
    private String id;
    
    // å»ºæ§‹å­
    public Document(String content) {
        this(content, new HashMap<>());
    }
    
    public Document(String content, Map<String, Object> metadata) {
        this.content = content;
        this.metadata = metadata;
        this.id = UUID.randomUUID().toString();
    }
    
    // getter/setter æ–¹æ³•
    public String getContent() { return content; }
    public Map<String, Object> getMetadata() { return metadata; }
    public String getId() { return id; }
}
```

### ETL ç®¡é“çµ„åˆ

```java
/**
 * ETL ç®¡é“ç¯„ä¾‹
 */
@Component
@Slf4j
public class EtlPipelineService {
    
    private final VectorStore vectorStore;
    
    public EtlPipelineService(VectorStore vectorStore) {
        this.vectorStore = vectorStore;
    }
    
    /**
     * åŸºæœ¬ ETL æµç¨‹
     */
    public void processDocuments(Resource resource) {
        // 1. Extract - ä½¿ç”¨é©ç•¶çš„ DocumentReader
        DocumentReader reader = createReader(resource);
        List<Document> documents = reader.read();
        
        // 2. Transform - ä½¿ç”¨ DocumentTransformer
        TokenTextSplitter splitter = new TokenTextSplitter();
        List<Document> transformedDocs = splitter.transform(documents);
        
        // 3. Load - å¯«å…¥å‘é‡è³‡æ–™åº«
        vectorStore.write(transformedDocs);
        
        log.info("Processed {} documents through ETL pipeline", documents.size());
    }
    
    /**
     * å‡½æ•¸å¼é¢¨æ ¼çš„ ETL æµç¨‹
     */
    public void processFunctionalStyle(Resource resource) {
        DocumentReader reader = createReader(resource);
        TokenTextSplitter splitter = new TokenTextSplitter();
        
        // å‡½æ•¸å¼çµ„åˆ
        vectorStore.accept(splitter.apply(reader.get()));
    }
    
    private DocumentReader createReader(Resource resource) {
        String filename = resource.getFilename();
        if (filename != null) {
            if (filename.endsWith(".pdf")) {
                return new PagePdfDocumentReader(resource);
            } else if (filename.endsWith(".txt")) {
                return new TextReader(resource);
            } else if (filename.endsWith(".docx") || filename.endsWith(".doc")) {
                return new TikaDocumentReader(resource);
            }
        }
        throw new IllegalArgumentException("Unsupported file type: " + filename);
    }
}
```

---

## 7.3.4 Spring AI DocumentReader å¯¦ç¾

### PDF æ–‡æª”è®€å–å™¨

#### ä¾è³´é…ç½®

é¦–å…ˆæ·»åŠ  PDF æ–‡æª”è®€å–å™¨ä¾è³´ï¼š

```xml
<dependency>
    <groupId>org.springframework.ai</groupId>
    <artifactId>spring-ai-pdf-document-reader</artifactId>
</dependency>
```

#### PagePdfDocumentReader å¯¦ç¾

```java
/**
 * PDF é é¢æ–‡æª”è®€å–å™¨ç¯„ä¾‹
 */
@Component
@Slf4j
public class PdfDocumentService {
    
    
    /**
     * åŸºæœ¬ PDF æ–‡æª”è®€å–
     */
    public List<Document> readPdfDocument(Resource pdfResource) {
        log.info("Reading PDF document: {}", pdfResource.getFilename());
        
        PagePdfDocumentReader pdfReader = new PagePdfDocumentReader(pdfResource,
            PdfDocumentReaderConfig.builder()
                .withPageTopMargin(0)
                .withPageExtractedTextFormatter(ExtractedTextFormatter.builder()
                    .withNumberOfTopTextLinesToDelete(0)
                    .build())
                .withPagesPerDocument(1) // æ¯é ä¸€å€‹æ–‡æª”
                .build());
        
        return pdfReader.read();
    }
    
    /**
     * ä½¿ç”¨æ®µè½åˆ†å‰²çš„ PDF è®€å–
     */
    public List<Document> readPdfWithParagraphs(Resource pdfResource) {
        log.info("Reading PDF with paragraph splitting: {}", pdfResource.getFilename());
        
        ParagraphPdfDocumentReader pdfReader = new ParagraphPdfDocumentReader(pdfResource,
            PdfDocumentReaderConfig.builder()
                .withPageTopMargin(0)
                .withPageExtractedTextFormatter(ExtractedTextFormatter.builder()
                    .withNumberOfTopTextLinesToDelete(0)
                    .build())
                .build());
        
        return pdfReader.read();
    }
    
    /**
     * é€²éš PDF é…ç½®ç¯„ä¾‹
     */
    public List<Document> readPdfWithAdvancedConfig(Resource pdfResource) {
        PdfDocumentReaderConfig config = PdfDocumentReaderConfig.builder()
            .withPageTopMargin(50)  // é é¢ä¸Šé‚Šè·
            .withPageBottomMargin(50) // é é¢ä¸‹é‚Šè·
            .withPageExtractedTextFormatter(
                ExtractedTextFormatter.builder()
                    .withNumberOfTopTextLinesToDelete(2) // åˆªé™¤é ‚éƒ¨è¡Œæ•¸
                    .withNumberOfBottomTextLinesToDelete(1) // åˆªé™¤åº•éƒ¨è¡Œæ•¸
                    .build())
            .withPagesPerDocument(3) // æ¯3é åˆä½µç‚ºä¸€å€‹æ–‡æª”
            .build();
        
        PagePdfDocumentReader pdfReader = new PagePdfDocumentReader(pdfResource, config);
        return pdfReader.read();
    }
    
    /**
     * PDF æ–‡æª”è™•ç†çš„å®Œæ•´ç¯„ä¾‹
     */
    public void processMultiplePdfs(List<Resource> pdfResources) {
        for (Resource resource : pdfResources) {
            try {
                List<Document> documents = readPdfDocument(resource);
                
                // æ·»åŠ è‡ªå®šç¾©å…ƒè³‡æ–™
                documents.forEach(doc -> {
                    doc.getMetadata().put("source_file", resource.getFilename());
                    doc.getMetadata().put("processed_at", LocalDateTime.now().toString());
                    doc.getMetadata().put("content_type", "PDF");
                });
                
                log.info("Successfully processed PDF: {} with {} documents", 
                    resource.getFilename(), documents.size());
                
            } catch (Exception e) {
                log.error("Failed to process PDF: {}", resource.getFilename(), e);
            }
        }
    }
    
}

/**
 * PDF é…ç½®é¡åˆ¥
 */
@Configuration
public class PdfReaderConfiguration {
    
    /**
     * é»˜èª PDF æ–‡æª”è®€å–å™¨
     */
    @Bean
    public PagePdfDocumentReader defaultPdfReader(@Value("classpath:sample.pdf") Resource pdfResource) {
        return new PagePdfDocumentReader(pdfResource);
    }
    
    /**
     * è‡ªå®šç¾©é…ç½®çš„ PDF æ–‡æª”è®€å–å™¨
     */
    @Bean("customPdfReader")
    public PagePdfDocumentReader customPdfReader(@Value("classpath:complex.pdf") Resource pdfResource) {
        PdfDocumentReaderConfig config = PdfDocumentReaderConfig.builder()
            .withPageTopMargin(100)
            .withPageBottomMargin(100)
            .withPageExtractedTextFormatter(
                ExtractedTextFormatter.builder()
                    .withNumberOfTopTextLinesToDelete(3)
                    .withNumberOfBottomTextLinesToDelete(2)
                    .withLeftAlignment(true)
                    .build())
            .withPagesPerDocument(2)
            .build();
        
        return new PagePdfDocumentReader(pdfResource, config);
    }
}
```

### Tika æ–‡æª”è®€å–å™¨

#### ä¾è³´é…ç½®

```xml
<dependency>
    <groupId>org.springframework.ai</groupId>
    <artifactId>spring-ai-tika-document-reader</artifactId>
</dependency>
```

#### TikaDocumentReader å¯¦ç¾

```java
/**
 * ä½¿ç”¨ Tika è™•ç†å¤šç¨®æ–‡æª”æ ¼å¼
 */
@Component
@Slf4j
public class TikaDocumentService {
    
    
    /**
     * ä½¿ç”¨ TikaDocumentReader è®€å– Word æ–‡æª”
     */
    public List<Document> readWordDocument(Resource wordResource) {
        log.info("Reading Word document with Tika: {}", wordResource.getFilename());
        
        TikaDocumentReader tikaReader = new TikaDocumentReader(wordResource);
        List<Document> documents = tikaReader.read();
        
        // æ·»åŠ è‡ªå®šç¾©å…ƒè³‡æ–™
        documents.forEach(doc -> {
            doc.getMetadata().put("document_type", "WORD");
            doc.getMetadata().put("source_file", wordResource.getFilename());
            doc.getMetadata().put("processed_with", "Apache Tika");
        });
        
        return documents;
    }
    
    /**
     * è®€å– PowerPoint æ–‡æª”
     */
    public List<Document> readPowerPointDocument(Resource pptResource) {
        log.info("Reading PowerPoint document with Tika: {}", pptResource.getFilename());
        
        TikaDocumentReader tikaReader = new TikaDocumentReader(pptResource);
        List<Document> documents = tikaReader.read();
        
        documents.forEach(doc -> {
            doc.getMetadata().put("document_type", "POWERPOINT");
            doc.getMetadata().put("source_file", pptResource.getFilename());
        });
        
        return documents;
    }
    
    /**
     * è®€å–å¤šç¨®æ ¼å¼çš„æ–‡æª”
     */
    public List<Document> readDocuments(List<Resource> resources) {
        List<Document> allDocuments = new ArrayList<>();
        
        for (Resource resource : resources) {
            try {
                String filename = resource.getFilename();
                if (filename != null) {
                    List<Document> docs;
                    
                    if (filename.toLowerCase().matches(".*\\.(docx?|pptx?|xlsx?)$")) {
                        // Microsoft Office æ–‡æª”
                        docs = readOfficeDocument(resource);
                    } else if (filename.toLowerCase().endsWith(".pdf")) {
                        // PDF æ–‡æª”ä½¿ç”¨å°ˆç”¨è®€å–å™¨
                        continue; // PDF ç”± PdfDocumentService è™•ç†
                    } else {
                        // å…¶ä»–æ ¼å¼å˜—è©¦ç”¨ Tika
                        docs = readGenericDocument(resource);
                    }
                    
                    allDocuments.addAll(docs);
                    log.info("Processed {} documents from {}", docs.size(), filename);
                }
            } catch (Exception e) {
                log.error("Failed to process document: {}", resource.getFilename(), e);
            }
        }
        
        return allDocuments;
    }
    
    private List<Document> readOfficeDocument(Resource resource) {
        TikaDocumentReader tikaReader = new TikaDocumentReader(resource);
        return tikaReader.read();
    }
    
    private List<Document> readGenericDocument(Resource resource) {
        TikaDocumentReader tikaReader = new TikaDocumentReader(resource);
        return tikaReader.read();
    }
    
}

/**
 * Text æ–‡æª”è®€å–å™¨ç¯„ä¾‹
 */
@Component
@Slf4j
public class TextDocumentService {
    
    /**
     * è®€å–ç´”æ–‡å­—æª”æ¡ˆ
     */
    public List<Document> readTextFile(Resource textResource) {
        log.info("Reading text file: {}", textResource.getFilename());
        
        TextReader textReader = new TextReader(textResource);
        
        // è¨­ç½®å­—ç¬¦ç·¨ç¢¼
        textReader.setCharset(StandardCharsets.UTF_8);
        
        // æ·»åŠ è‡ªå®šç¾©å…ƒè³‡æ–™
        textReader.getCustomMetadata().put("filename", textResource.getFilename());
        textReader.getCustomMetadata().put("content_type", "TEXT");
        
        return textReader.read();
    }
    
    /**
     * è®€å– Markdown æ–‡ä»¶
     */
    public List<Document> readMarkdownFile(Resource markdownResource) {
        log.info("Reading markdown file: {}", markdownResource.getFilename());
        
        MarkdownDocumentReaderConfig config = MarkdownDocumentReaderConfig.builder()
            .withHorizontalRuleCreateDocument(true) // æ°´å¹³ç·šåˆ†å‰²æ–‡æª”
            .withIncludeCodeBlock(true)  // åŒ…å«ç¨‹å¼ç¢¼å€å¡Š
            .withIncludeBlockquote(true) // åŒ…å«å¼•ç”¨å€å¡Š
            .withAdditionalMetadata("filename", markdownResource.getFilename())
            .build();
        
        MarkdownDocumentReader markdownReader = new MarkdownDocumentReader(markdownResource, config);
        return markdownReader.read();
    }
    
    /**
     * è®€å– JSON æ–‡ä»¶
     */
    public List<Document> readJsonFile(Resource jsonResource, String... jsonKeysToUse) {
        log.info("Reading JSON file: {}", jsonResource.getFilename());
        
        JsonReader jsonReader = new JsonReader(jsonResource, jsonKeysToUse);
        return jsonReader.read();
    }
    
    /**
     * ä½¿ç”¨ JSON Pointer è®€å–ç‰¹å®šéƒ¨åˆ†
     */
    public List<Document> readJsonWithPointer(Resource jsonResource, String jsonPointer, String... jsonKeysToUse) {
        log.info("Reading JSON file with pointer {}: {}", jsonPointer, jsonResource.getFilename());
        
        JsonReader jsonReader = new JsonReader(jsonResource, jsonKeysToUse);
        return jsonReader.get(jsonPointer);
    }
    
}

/**
 * HTML æ–‡æª”è®€å–å™¨ç¯„ä¾‹
 */
@Component
@Slf4j
public class HtmlDocumentService {
    
    /**
     * è®€å– HTML æ–‡ä»¶
     */
    public List<Document> readHtmlFile(Resource htmlResource) {
        log.info("Reading HTML file: {}", htmlResource.getFilename());
        
        JsoupDocumentReaderConfig config = JsoupDocumentReaderConfig.builder()
            .selector("article p, div.content") // CSS é¸æ“‡å™¨
            .charset("UTF-8")
            .includeLinkUrls(true) // åŒ…å«é€£çµ URL
            .metadataTags(List.of("description", "keywords", "author")) // æå– meta æ¨™ç±¤
            .additionalMetadata("source", htmlResource.getFilename())
            .build();
        
        JsoupDocumentReader htmlReader = new JsoupDocumentReader(htmlResource, config);
        return htmlReader.read();
    }
    
    /**
     * è®€å–ç¶²é  URL
     */
    public List<Document> readWebPage(String url) {
        log.info("Reading web page: {}", url);
        
        try {
            Resource urlResource = new UrlResource(url);
            
            JsoupDocumentReaderConfig config = JsoupDocumentReaderConfig.builder()
                .selector("body") // æå–æ•´å€‹ body å…§å®¹
                .allElements(false) // ä¸ä½¿ç”¨æ‰€æœ‰å…ƒç´ 
                .groupByElement(false) // ä¸æŒ‰å…ƒç´ åˆ†çµ„
                .includeLinkUrls(false)
                .metadataTags(List.of("title", "description"))
                .additionalMetadata("url", url)
                .build();
            
            JsoupDocumentReader htmlReader = new JsoupDocumentReader(urlResource, config);
            return htmlReader.read();
            
        } catch (Exception e) {
            log.error("Failed to read web page: {}", url, e);
            return Collections.emptyList();
        }
    }
    
}
```

---

## 7.3.5 æ–‡æª”è½‰æ›å™¨ (DocumentTransformer)

### TokenTextSplitter - æ–‡æœ¬åˆ†å‰²å™¨

```java
/**
 * ä½¿ç”¨ TokenTextSplitter åˆ†å‰²æ–‡æª”
 */
@Component
@Slf4j
public class DocumentTransformService {
    
    /**
     * åŸºæœ¬æ–‡æœ¬åˆ†å‰²
     */
    public List<Document> splitDocuments(List<Document> documents) {
        TokenTextSplitter splitter = new TokenTextSplitter();
        return splitter.transform(documents);
    }
    
    /**
     * è‡ªå®šç¾©åˆ†å‰²åƒæ•¸
     */
    public List<Document> splitDocumentsCustom(List<Document> documents) {
        // defaultChunkSize: 1000, minChunkSizeChars: 400, 
        // minChunkLengthToEmbed: 10, maxNumChunks: 5000, keepSeparator: true
        TokenTextSplitter splitter = new TokenTextSplitter(1000, 400, 10, 5000, true);
        return splitter.transform(documents);
    }
    
    /**
     * å…§å®¹æ ¼å¼è½‰æ›å™¨
     */
    public List<Document> formatContent(List<Document> documents) {
        ContentFormatTransformer formatter = new ContentFormatTransformer();
        return formatter.transform(documents);
    }
}

/**
 * å…ƒè³‡æ–™å¢å¼·æœå‹™
 */
@Component
@Slf4j
public class MetadataEnrichmentService {
    
    private final ChatModel chatModel;
    
    public MetadataEnrichmentService(ChatModel chatModel) {
        this.chatModel = chatModel;
    }
    
    /**
     * é—œéµå­—æå–å™¨
     */
    public List<Document> enrichWithKeywords(List<Document> documents, int keywordCount) {
        KeywordMetadataEnricher enricher = new KeywordMetadataEnricher(chatModel, keywordCount);
        return enricher.transform(documents);
    }
    
    /**
     * æ‘˜è¦ç”¢ç”Ÿå™¨
     */
    public List<Document> enrichWithSummary(List<Document> documents) {
        SummaryMetadataEnricher enricher = new SummaryMetadataEnricher(
            chatModel, 
            List.of(SummaryMetadataEnricher.SummaryType.CURRENT,
                   SummaryMetadataEnricher.SummaryType.PREVIOUS,
                   SummaryMetadataEnricher.SummaryType.NEXT)
        );
        return enricher.transform(documents);
    }
    
}

/**
 * æ–‡æª”è½‰æ›ç®¡é“ç¯„ä¾‹
 */
@Component
@Slf4j
public class DocumentProcessingPipeline {
    
    private final ChatModel chatModel;
    
    public DocumentProcessingPipeline(ChatModel chatModel) {
        this.chatModel = chatModel;
    }
    
    /**
     * å®Œæ•´çš„æ–‡æª”è™•ç†ç®¡é“
     */
    public List<Document> processDocuments(List<Document> documents) {
        log.info("Starting document processing pipeline with {} documents", documents.size());
        
        // 1. æ–‡æœ¬åˆ†å‰²
        TokenTextSplitter splitter = new TokenTextSplitter(800, 350, 5, 10000, true);
        List<Document> splitDocs = splitter.transform(documents);
        log.info("Split into {} chunks", splitDocs.size());
        
        // 2. å…§å®¹æ ¼å¼åŒ–
        ContentFormatTransformer formatter = new ContentFormatTransformer();
        List<Document> formattedDocs = formatter.transform(splitDocs);
        
        // 3. é—œéµå­—æå–
        KeywordMetadataEnricher keywordEnricher = new KeywordMetadataEnricher(chatModel, 5);
        List<Document> keywordEnrichedDocs = keywordEnricher.transform(formattedDocs);
        
        // 4. æ‘˜è¦ç”¢ç”Ÿ
        SummaryMetadataEnricher summaryEnricher = new SummaryMetadataEnricher(
            chatModel,
            List.of(SummaryMetadataEnricher.SummaryType.CURRENT)
        );
        List<Document> enrichedDocs = summaryEnricher.transform(keywordEnrichedDocs);
        
        log.info("Completed document processing pipeline with {} enriched documents", enrichedDocs.size());
        return enrichedDocs;
    }
    
}
    
```

---

## 7.3.6 æ–‡æª”å¯«å…¥å™¨ (DocumentWriter)

### VectorStore - å‘é‡è³‡æ–™åº«å¯«å…¥å™¨

```java
/**
 * å‘é‡è³‡æ–™åº«å¯«å…¥æœå‹™
 */
@Component
@Slf4j
public class VectorStoreService {
    
    private final VectorStore vectorStore;
    private final EmbeddingModel embeddingModel;
    
    public VectorStoreService(VectorStore vectorStore, EmbeddingModel embeddingModel) {
        this.vectorStore = vectorStore;
        this.embeddingModel = embeddingModel;
    }
    
    /**
     * å°‡æ–‡æª”å¯«å…¥å‘é‡è³‡æ–™åº«
     */
    public void writeDocuments(List<Document> documents) {
        log.info("Writing {} documents to vector store", documents.size());
        
        try {
            // ç›´æ¥ä½¿ç”¨ VectorStore.write() æ–¹æ³•
            vectorStore.write(documents);
            log.info("Successfully wrote {} documents to vector store", documents.size());
        } catch (Exception e) {
            log.error("Failed to write documents to vector store", e);
            throw new RuntimeException("Vector store write failed", e);
        }
    }
    
    /**
     * æ‰¹æ¬¡å¯«å…¥æ–‡æª”
     */
    public void writeBatchDocuments(List<Document> documents, int batchSize) {
        log.info("Writing {} documents in batches of {}", documents.size(), batchSize);
        
        for (int i = 0; i < documents.size(); i += batchSize) {
            int endIndex = Math.min(i + batchSize, documents.size());
            List<Document> batch = documents.subList(i, endIndex);
            
            try {
                vectorStore.write(batch);
                log.info("Wrote batch {}-{} of {} documents", i + 1, endIndex, documents.size());
            } catch (Exception e) {
                log.error("Failed to write batch {}-{}", i + 1, endIndex, e);
                throw new RuntimeException("Batch write failed", e);
            }
        }
    }
    
    /**
     * ä½¿ç”¨æœå°‹å¼•æ“æŸ¥è©¢æ–‡æª”
     */
    public List<Document> searchSimilarDocuments(String query, int k) {
        log.info("Searching for {} similar documents with query: {}", k, query);
        
        try {
            SearchRequest searchRequest = SearchRequest.query(query).withTopK(k);
            return vectorStore.similaritySearch(searchRequest);
        } catch (Exception e) {
            log.error("Failed to search similar documents", e);
            return Collections.emptyList();
        }
    }
    
    /**
     * ä½¿ç”¨é–¾å€¼éæ¿¾çš„æœå°‹
     */
    public List<Document> searchWithThreshold(String query, int k, double threshold) {
        log.info("Searching with threshold {}: {}", threshold, query);
        
        try {
            SearchRequest searchRequest = SearchRequest.query(query)
                .withTopK(k)
                .withSimilarityThreshold(threshold);
            return vectorStore.similaritySearch(searchRequest);
        } catch (Exception e) {
            log.error("Failed to search with threshold", e);
            return Collections.emptyList();
        }
    }
    
}

/**
 * æ–‡ä»¶å¯«å…¥å™¨ç¯„ä¾‹
 */
@Component
@Slf4j
public class FileWriterService {
    
    /**
     * å°‡æ–‡æª”å¯«å…¥æ–‡ä»¶
     */
    public void writeDocumentsToFile(List<Document> documents, String filename) {
        log.info("Writing {} documents to file: {}", documents.size(), filename);
        
        FileDocumentWriter writer = new FileDocumentWriter(filename, true, MetadataMode.ALL, false);
        writer.write(documents);
        
        log.info("Successfully wrote documents to file: {}", filename);
    }
    
    /**
     * é™„åŠ æ–‡æª”æ¨™è¨˜çš„æ–‡ä»¶å¯«å…¥
     */
    public void writeDocumentsWithMarkers(List<Document> documents, String filename) {
        log.info("Writing {} documents with markers to file: {}", documents.size(), filename);
        
        FileDocumentWriter writer = new FileDocumentWriter(
            filename, 
            true,  // withDocumentMarkers
            MetadataMode.ALL, 
            true   // append
        );
        writer.write(documents);
    }
    
}

/**
 * å®Œæ•´ ETL ç®¡é“ç¯„ä¾‹
 */
@Component
@Slf4j
public class CompleteEtlPipeline {
    
    private final VectorStore vectorStore;
    private final ChatModel chatModel;
    
    public CompleteEtlPipeline(VectorStore vectorStore, ChatModel chatModel) {
        this.vectorStore = vectorStore;
        this.chatModel = chatModel;
    }
    
    /**
     * å®Œæ•´çš„ ETL æµç¨‹
     */
    public void processDocumentsPipeline(List<Resource> resources) {
        log.info("Starting complete ETL pipeline with {} resources", resources.size());
        
        // Extract - è®€å–æ–‡æª”
        List<Document> documents = extractDocuments(resources);
        log.info("Extracted {} documents", documents.size());
        
        // Transform - è½‰æ›æ–‡æª”
        List<Document> transformedDocs = transformDocuments(documents);
        log.info("Transformed to {} document chunks", transformedDocs.size());
        
        // Load - å°‡æ–‡æª”è¼‰å…¥å‘é‡è³‡æ–™åº«
        loadDocuments(transformedDocs);
        log.info("Loaded documents to vector store");
        
        log.info("ETL pipeline completed successfully");
    }
    
    private List<Document> extractDocuments(List<Resource> resources) {
        List<Document> allDocs = new ArrayList<>();
        
        for (Resource resource : resources) {
            DocumentReader reader = createReader(resource);
            List<Document> docs = reader.read();
            allDocs.addAll(docs);
        }
        
        return allDocs;
    }
    
    private DocumentReader createReader(Resource resource) {
        String filename = resource.getFilename();
        if (filename != null) {
            String lowerFilename = filename.toLowerCase();
            if (lowerFilename.endsWith(".pdf")) {
                return new PagePdfDocumentReader(resource);
            } else if (lowerFilename.matches(".*\\.(docx?|pptx?|xlsx?)$")) {
                return new TikaDocumentReader(resource);
            } else if (lowerFilename.endsWith(".txt")) {
                return new TextReader(resource);
            } else if (lowerFilename.endsWith(".md")) {
                return new MarkdownDocumentReader(resource);
            } else if (lowerFilename.endsWith(".json")) {
                return new JsonReader(resource);
            }
        }
        throw new IllegalArgumentException("Unsupported file type: " + filename);
    }
    
    private List<Document> transformDocuments(List<Document> documents) {
        // 1. æ–‡æœ¬åˆ†å‰²
        TokenTextSplitter splitter = new TokenTextSplitter();
        List<Document> splitDocs = splitter.transform(documents);
        
        // 2. å…ƒè³‡æ–™å¢å¼·
        KeywordMetadataEnricher enricher = new KeywordMetadataEnricher(chatModel, 5);
        return enricher.transform(splitDocs);
    }
    
    private void loadDocuments(List<Document> documents) {
        vectorStore.write(documents);
    }
}
```

---

## ğŸ“ æœ¬ç« é‡é»å›é¡§

1. **ETL åŸºç¤æ¦‚å¿µ**ï¼šç†è§£äº† Extractã€Transformã€Load çš„å®Œæ•´æµç¨‹
2. **çŸ¥è­˜ä¾†æºåˆ†é¡**ï¼šæŒæ¡äº†ä¼æ¥­ä¸­å„ç¨®è³‡æ–™ä¾†æºçš„ç‰¹é»
3. **æå–å™¨æ¶æ§‹**ï¼šè¨­è¨ˆäº†å¯æ“´å±•çš„è³‡æ–™æå–å™¨ç³»çµ±
4. **æ–‡æª”æå–å¯¦ç¾**ï¼šå®Œæˆäº† PDFã€Word ç­‰æ–‡æª”çš„æå–å™¨
5. **ç¶²é å’Œè³‡æ–™åº«æå–**ï¼šå¯¦ç¾äº†ç¶²é å…§å®¹å’Œ SQL è³‡æ–™åº«çš„æå–

### æŠ€è¡“è¦é»ç¸½çµ

| æŠ€è¡“é» | é‡è¦æ€§ | å¯¦ç¾é›£åº¦ | ä¼æ¥­åƒ¹å€¼ |
|--------|--------|----------|----------|
| **ETL æ¶æ§‹è¨­è¨ˆ** | â­â­â­ | é«˜ | ç³»çµ±åŸºç¤ |
| **æ–‡æª”æå–** | â­â­â­ | ä¸­ | æ ¸å¿ƒåŠŸèƒ½ |
| **ç¶²é æå–** | â­â­ | ä¸­ | è³‡æ–™è±å¯Œæ€§ |
| **è³‡æ–™åº«æ•´åˆ** | â­â­â­ | ä¸­ | ä¼æ¥­æ•´åˆ |
| **å“è³ªæ§åˆ¶** | â­â­ | ä¸­ | è³‡æ–™å¯é æ€§ |

### æœ€ä½³å¯¦è¸å»ºè­°

1. **æ¨¡çµ„åŒ–è¨­è¨ˆ**ï¼šä½¿ç”¨å¯æ’æ‹”çš„æå–å™¨æ¶æ§‹ï¼Œä¾¿æ–¼æ“´å±•æ–°çš„è³‡æ–™ä¾†æº
2. **éŒ¯èª¤è™•ç†**ï¼šå¯¦ç¾å®Œå–„çš„éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶
3. **æ•ˆèƒ½å„ªåŒ–**ï¼šä½¿ç”¨æ‰¹æ¬¡è™•ç†å’Œä¸¦è¡Œè™•ç†æé«˜æå–æ•ˆç‡
4. **è³‡æ–™å“è³ª**ï¼šå»ºç«‹è³‡æ–™é©—è­‰å’Œæ¸…ç†æ©Ÿåˆ¶
5. **ç›£æ§å‘Šè­¦**ï¼šå¯¦æ–½å®Œæ•´çš„æå–éç¨‹ç›£æ§å’Œå‘Šè­¦

### ä¸‹ä¸€æ­¥å­¸ç¿’æ–¹å‘

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘å€‘å°‡å­¸ç¿’ ETL çš„ä¸­ç¯‡ - æ“·å–é€²éšæ–‡ä»¶é¡å‹ï¼ŒåŒ…æ‹¬ï¼š
- Excel å’Œ PowerPoint æ–‡æª”è™•ç†
- åœ–åƒå’Œå¤šåª’é«”å…§å®¹æå–
- å£“ç¸®æª”æ¡ˆå’Œè¤‡åˆæ–‡æª”è™•ç†
- ç‰¹æ®Šæ ¼å¼æ–‡æª”çš„è™•ç†æŠ€å·§

---

**åƒè€ƒè³‡æ–™ï¼š**
- [Apache POI Documentation](https://poi.apache.org/)
- [Apache PDFBox Guide](https://pdfbox.apache.org/)
- [Jsoup HTML Parser](https://jsoup.org/)
- [Spring Data JDBC](https://spring.io/projects/spring-data-jdbc)