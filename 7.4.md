# 7.4 ETL (中) - 擷取進階文件類型

> **本章重點**：深入探討進階文件類型的提取技術，掌握 Excel、PowerPoint、圖像、壓縮檔案等複雜格式的處理方法，建立完整的多媒體內容提取能力。

## 🎯 學習目標

完成本章學習後，您將能夠：

- 🎯 **處理 Excel 試算表**：提取表格資料、圖表和公式內容
- 🎯 **解析 PowerPoint 簡報**：提取投影片文字、圖片和註解
- 🎯 **處理圖像文件**：使用 OCR 技術提取圖像中的文字
- 🎯 **解壓縮檔案**：處理 ZIP、RAR 等壓縮格式的批次提取
- 🎯 **整合多媒體內容**：建立統一的多媒體內容處理管道

---

## 7.4.1 Excel 試算表提取器

### Excel 文件特點與挑戰

**Excel 文件的複雜性**：
- 📊 **多工作表結構**：一個文件可包含多個工作表
- 📈 **豐富的資料類型**：數值、文字、日期、公式、圖表
- 🎨 **格式化資訊**：字體、顏色、邊框、合併儲存格
- 📋 **元資料豐富**：作者、創建時間、註解、超連結

### Excel 提取器實現

Spring AI 使用 **TikaDocumentReader** 來處理 Excel 文件，這是推薦的方式：

#### 依賴配置

```xml
<dependency>
    <groupId>org.springframework.ai</groupId>
    <artifactId>spring-ai-tika-document-reader</artifactId>
</dependency>
```

#### 基本用法

```java
/**
 * Excel 文件讀取器
 */
@Component
@Slf4j
public class ExcelDocumentReader {
    
    /**
     * 讀取 Excel 文件
     */
    public List<Document> readExcelFile(Resource resource) {
        log.info("Reading Excel file: {}", resource.getFilename());
        
        try {
            TikaDocumentReader tikaReader = new TikaDocumentReader(resource);
            List<Document> documents = tikaReader.read();
            
            // 添加 Excel 特定的元資料
            documents.forEach(doc -> {
                doc.getMetadata().put("document_type", "EXCEL");
                doc.getMetadata().put("source_file", resource.getFilename());
                doc.getMetadata().put("extraction_method", "TIKA");
            });
            
            log.info("Successfully extracted {} documents from Excel file", documents.size());
            return documents;
            
        } catch (Exception e) {
            log.error("Failed to read Excel file: {}", resource.getFilename(), e);
            throw new RuntimeException("Excel 文件讀取失敗", e);
        }
    }
    
    /**
     * 批次處理多個 Excel 文件
     */
    public List<Document> readMultipleExcelFiles(List<Resource> resources) {
        List<Document> allDocuments = new ArrayList<>();
        
        for (Resource resource : resources) {
            try {
                List<Document> documents = readExcelFile(resource);
                allDocuments.addAll(documents);
            } catch (Exception e) {
                log.warn("Failed to process Excel file: {}", resource.getFilename(), e);
            }
        }
        
        return allDocuments;
    }
}

/**
 * ETL Pipeline 整合範例
 */
@Component
@Slf4j
public class ExcelETLProcessor {
    
    private final VectorStore vectorStore;
    private final TokenTextSplitter textSplitter;
    
    public ExcelETLProcessor(VectorStore vectorStore) {
        this.vectorStore = vectorStore;
        this.textSplitter = new TokenTextSplitter(500, 100, 5, 1000, true);
    }
    
    /**
     * 完整的 Excel ETL 流程
     */
    public void processExcelToVectorStore(Resource excelResource) {
        try {
            // 1. Extract: 讀取 Excel 文件
            TikaDocumentReader documentReader = new TikaDocumentReader(excelResource);
            List<Document> documents = documentReader.read();
            
            // 2. Transform: 文本分割和預處理
            List<Document> transformedDocuments = textSplitter.apply(documents);
            
            // 添加額外的元資料增強
            transformedDocuments.forEach(doc -> {
                doc.getMetadata().put("processing_timestamp", LocalDateTime.now().toString());
                doc.getMetadata().put("document_type", "EXCEL");
            });
            
            // 3. Load: 載入到向量資料庫
            vectorStore.write(transformedDocuments);
            
            log.info("成功處理 Excel 文件，共產生 {} 個文檔片段", transformedDocuments.size());
            
        } catch (Exception e) {
            log.error("Excel ETL 處理失敗", e);
            throw new RuntimeException("Excel ETL 處理失敗", e);
        }
    }
    
    /**
     * 批次處理多個 Excel 文件
     */
    public void processBatchExcelFiles(List<Resource> resources) {
        resources.parallelStream().forEach(resource -> {
            try {
                processExcelToVectorStore(resource);
            } catch (Exception e) {
                log.error("批次處理失敗: {}", resource.getFilename(), e);
            }
        });
    }
}
```

---

## 7.4.2 PowerPoint 簡報提取器

### PowerPoint 提取器實現

Spring AI 同樣使用 **TikaDocumentReader** 來處理 PowerPoint 文件：

```java
/**
 * PowerPoint 簡報讀取器
 */
@Component
@Slf4j
public class PowerPointDocumentReader {
    
    /**
     * 讀取 PowerPoint 文件
     */
    public List<Document> readPowerPointFile(Resource resource) {
        log.info("Reading PowerPoint file: {}", resource.getFilename());
        
        try {
            TikaDocumentReader tikaReader = new TikaDocumentReader(resource);
            List<Document> documents = tikaReader.read();
            
            // 添加 PowerPoint 特定的元資料
            documents.forEach(doc -> {
                doc.getMetadata().put("document_type", "POWERPOINT");
                doc.getMetadata().put("source_file", resource.getFilename());
                doc.getMetadata().put("extraction_method", "TIKA");
            });
            
            log.info("Successfully extracted {} documents from PowerPoint file", documents.size());
            return documents;
            
        } catch (Exception e) {
            log.error("Failed to read PowerPoint file: {}", resource.getFilename(), e);
            throw new RuntimeException("PowerPoint 文件讀取失敗", e);
        }
    }
}

/**
 * PowerPoint ETL Pipeline 整合範例
 */
@Component
@Slf4j
public class PowerPointETLProcessor {
    
    private final VectorStore vectorStore;
    private final TokenTextSplitter textSplitter;
    
    public PowerPointETLProcessor(VectorStore vectorStore) {
        this.vectorStore = vectorStore;
        this.textSplitter = new TokenTextSplitter(800, 200, 5, 1000, true);
    }
    
    /**
     * 完整的 PowerPoint ETL 流程
     */
    public void processPowerPointToVectorStore(Resource powerPointResource) {
        try {
            // 1. Extract: 讀取 PowerPoint 文件
            TikaDocumentReader documentReader = new TikaDocumentReader(powerPointResource);
            List<Document> documents = documentReader.read();
            
            // 2. Transform: 文本分割和預處理
            List<Document> transformedDocuments = textSplitter.apply(documents);
            
            // 添加額外的元資料增強
            transformedDocuments.forEach(doc -> {
                doc.getMetadata().put("processing_timestamp", LocalDateTime.now().toString());
                doc.getMetadata().put("document_type", "POWERPOINT");
            });
            
            // 3. Load: 載入到向量資料庫
            vectorStore.write(transformedDocuments);
            
            log.info("成功處理 PowerPoint 文件，共產生 {} 個文檔片段", transformedDocuments.size());
            
        } catch (Exception e) {
            log.error("PowerPoint ETL 處理失敗", e);
            throw new RuntimeException("PowerPoint ETL 處理失敗", e);
        }
    }
    
    /**
     * 批次處理多個簡報文件
     */
    public void processBatchPowerPointFiles(List<Resource> resources) {
        resources.parallelStream().forEach(resource -> {
            try {
                processPowerPointToVectorStore(resource);
            } catch (Exception e) {
                log.error("批次處理失敗: {}", resource.getFilename(), e);
            }
        });
    }
}
```

---

## 7.4.3 圖像文件 OCR 提取器

### OCR DocumentReader 實現

Spring AI 沒有內建的 OCR DocumentReader，但我們可以實現一個符合 Spring AI 架構的 OCR DocumentReader：

```java
/**
 * OCR 圖像文字提取器 (實現 Spring AI DocumentReader 接口)
 */
@Component
@Slf4j
public class ImageOCRDocumentReader implements DocumentReader {
    
    private final Resource resource;
    private final TesseractOCRService ocrService;
    
    public ImageOCRDocumentReader(Resource resource, TesseractOCRService ocrService) {
        this.resource = resource;
        this.ocrService = ocrService;
    }
    
    @Override
    public List<Document> read() {
        log.info("Extracting text from image: {}", resource.getFilename());
        
        try (InputStream inputStream = resource.getInputStream()) {
            
            // 讀取圖像
            BufferedImage image = ImageIO.read(inputStream);
            
            if (image != null) {
                // 圖像預處理
                BufferedImage processedImage = preprocessImage(image);
                
                // OCR 文字識別
                String extractedText = ocrService.extractText(processedImage);
                
                if (extractedText != null && !extractedText.trim().isEmpty()) {
                    Map<String, Object> metadata = createImageMetadata(image);
                    return List.of(new Document(extractedText.trim(), metadata));
                }
            }
            
            return List.of();
            
        } catch (Exception e) {
            log.error("Failed to extract text from image: {}", resource.getFilename(), e);
            throw new RuntimeException("OCR 文字提取失敗", e);
        }
    }
    
    /**
     * 圖像預處理
     */
    private BufferedImage preprocessImage(BufferedImage originalImage) {
        // 轉換為灰階
        BufferedImage grayImage = new BufferedImage(
            originalImage.getWidth(), 
            originalImage.getHeight(), 
            BufferedImage.TYPE_BYTE_GRAY
        );
        
        Graphics2D g2d = grayImage.createGraphics();
        g2d.drawImage(originalImage, 0, 0, null);
        g2d.dispose();
        
        // 增強對比度
        return enhanceContrast(grayImage);
    }
    
    /**
     * 增強對比度
     */
    private BufferedImage enhanceContrast(BufferedImage image) {
        RescaleOp rescaleOp = new RescaleOp(1.2f, 15, null);
        return rescaleOp.filter(image, null);
    }
    
    private Map<String, Object> createImageMetadata(BufferedImage image) {
        Map<String, Object> metadata = new HashMap<>();
        metadata.put("document_type", "IMAGE_OCR");
        metadata.put("source_file", resource.getFilename());
        metadata.put("image_width", image.getWidth());
        metadata.put("image_height", image.getHeight());
        metadata.put("extraction_method", "TESSERACT_OCR");
        metadata.put("extraction_timestamp", LocalDateTime.now().toString());
        
        return metadata;
    }
}

/**
 * Tesseract OCR 服務
 */
@Service
@Slf4j
public class TesseractOCRService {
    
    private final Tesseract tesseract;
    
    public TesseractOCRService() {
        this.tesseract = new Tesseract();
        configureTesseract();
    }
    
    private void configureTesseract() {
        try {
            // 設定 Tesseract 資料路徑（根據作業系統調整）
            String os = System.getProperty("os.name").toLowerCase();
            if (os.contains("win")) {
                tesseract.setDatapath("C:\\Program Files\\Tesseract-OCR\\tessdata");
            } else {
                tesseract.setDatapath("/usr/share/tesseract-ocr/4.00/tessdata");
            }
            
            // 設定語言（支援繁體中文和英文）
            tesseract.setLanguage("chi_tra+eng");
            
            // 設定 OCR 引擎模式
            tesseract.setOcrEngineMode(1); // LSTM OCR Engine
            
            // 設定頁面分割模式
            tesseract.setPageSegMode(1); // Automatic page segmentation with OSD
            
        } catch (Exception e) {
            log.error("Failed to configure Tesseract", e);
        }
    }
    
    /**
     * 從圖像提取文字
     */
    public String extractText(BufferedImage image) {
        try {
            return tesseract.doOCR(image);
        } catch (Exception e) {
            log.error("OCR text extraction failed", e);
            return "";
        }
    }
}

/**
 * OCR ETL Pipeline 處理器
 */
@Component
@Slf4j
public class ImageOCRETLProcessor {
    
    private final VectorStore vectorStore;
    private final TesseractOCRService ocrService;
    private final TokenTextSplitter textSplitter;
    
    public ImageOCRETLProcessor(VectorStore vectorStore, TesseractOCRService ocrService) {
        this.vectorStore = vectorStore;
        this.ocrService = ocrService;
        this.textSplitter = new TokenTextSplitter(800, 200, 10, 1000, true);
    }
    
    /**
     * 完整的圖像 OCR ETL 流程
     */
    public void processImageOCRToVectorStore(Resource imageResource) {
        try {
            // 1. Extract: 使用 OCR 讀取圖像文字
            ImageOCRDocumentReader documentReader = new ImageOCRDocumentReader(imageResource, ocrService);
            List<Document> documents = documentReader.read();
            
            if (!documents.isEmpty()) {
                // 2. Transform: 文本分割（OCR 文字通常較短，可能不需要分割）
                List<Document> transformedDocuments = documents.stream()
                    .filter(doc -> doc.getContent().trim().length() > 10) // 過濾太短的文字
                    .collect(Collectors.toList());
                
                // 如果文字較長，進行分割
                if (transformedDocuments.stream().anyMatch(doc -> doc.getContent().length() > 500)) {
                    transformedDocuments = textSplitter.apply(transformedDocuments);
                }
                
                // 3. Load: 載入到向量資料庫
                vectorStore.write(transformedDocuments);
                
                log.info("成功處理圖像 OCR，共產生 {} 個文檔片段", transformedDocuments.size());
            } else {
                log.warn("圖像 {} 未提取到任何文字", imageResource.getFilename());
            }
            
        } catch (Exception e) {
            log.error("圖像 OCR ETL 處理失敗", e);
            throw new RuntimeException("圖像 OCR ETL 處理失敗", e);
        }
    }
    
    /**
     * 批次處理多個圖像文件
     */
    public void processBatchImageFiles(List<Resource> imageResources) {
        imageResources.parallelStream().forEach(resource -> {
            try {
                processImageOCRToVectorStore(resource);
            } catch (Exception e) {
                log.error("批次 OCR 處理失敗: {}", resource.getFilename(), e);
            }
        });
    }
}
```

---

## 7.4.4 壓縮檔案處理器

### 壓縮檔案 DocumentReader 實現

Spring AI 沒有內建的壓縮檔案處理器，但我們可以實現一個符合 Spring AI 架構的壓縮檔案處理器：

```java
/**
 * 壓縮檔案 DocumentReader (實現 Spring AI DocumentReader 接口)
 */
@Component
@Slf4j
public class ArchiveDocumentReader implements DocumentReader {
    
    private final Resource archiveResource;
    private final DocumentReaderFactory documentReaderFactory;
    
    public ArchiveDocumentReader(Resource archiveResource, DocumentReaderFactory documentReaderFactory) {
        this.archiveResource = archiveResource;
        this.documentReaderFactory = documentReaderFactory;
    }
    
    @Override
    public List<Document> read() {
        log.info("Extracting archive file: {}", archiveResource.getFilename());
        
        String fileName = archiveResource.getFilename().toLowerCase();
        
        try {
            if (fileName.endsWith(".zip")) {
                return extractZipFile();
            } else {
                log.warn("Unsupported archive format: {}", fileName);
                return List.of();
            }
        } catch (Exception e) {
            log.error("Failed to extract archive: {}", archiveResource.getFilename(), e);
            throw new RuntimeException("壓縮檔案處理失敗", e);
        }
    }
    
    /**
     * 提取 ZIP 檔案
     */
    private List<Document> extractZipFile() throws IOException {
        List<Document> documents = new ArrayList<>();
        
        try (InputStream inputStream = archiveResource.getInputStream();
             ZipInputStream zipInputStream = new ZipInputStream(inputStream)) {
            
            ZipEntry entry;
            while ((entry = zipInputStream.getNextEntry()) != null) {
                
                if (!entry.isDirectory() && isSupportedFileType(entry.getName())) {
                    try {
                        List<Document> entryDocuments = extractZipEntry(zipInputStream, entry);
                        documents.addAll(entryDocuments);
                    } catch (Exception e) {
                        log.warn("Failed to extract zip entry {}: {}", entry.getName(), e.getMessage());
                    }
                }
                
                zipInputStream.closeEntry();
            }
        }
        
        return documents;
    }
    
    /**
     * 提取 ZIP 條目
     */
    private List<Document> extractZipEntry(ZipInputStream zipInputStream, ZipEntry entry) throws IOException {
        String entryName = entry.getName();
        
        // 讀取條目內容到記憶體
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        byte[] buffer = new byte[8192];
        int bytesRead;
        
        while ((bytesRead = zipInputStream.read(buffer)) != -1) {
            baos.write(buffer, 0, bytesRead);
        }
        
        byte[] entryData = baos.toByteArray();
        
        // 建立記憶體資源
        Resource entryResource = new ByteArrayResource(entryData, entryName);
        
        try {
            // 根據文件類型選擇適當的 DocumentReader
            DocumentReader reader = documentReaderFactory.createReader(entryResource);
            List<Document> documents = reader.read();
            
            // 添加壓縮檔案相關的元資料
            documents.forEach(doc -> {
                doc.getMetadata().put("archive_source", archiveResource.getFilename());
                doc.getMetadata().put("archive_entry", entryName);
                doc.getMetadata().put("extraction_method", "ZIP_ARCHIVE");
            });
            
            return documents;
            
        } catch (Exception e) {
            log.warn("No suitable reader found for archive entry: {}", entryName);
            return List.of();
        }
    }
    
    /**
     * 檢查檔案類型是否支援
     */
    private boolean isSupportedFileType(String fileName) {
        String lowerFileName = fileName.toLowerCase();
        
        // 支援的檔案類型
        List<String> supportedTypes = Arrays.asList(
            ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx",
            ".txt", ".md", ".html", ".htm", ".xml", ".json"
        );
        
        return supportedTypes.stream().anyMatch(lowerFileName::endsWith);
    }
}

/**
 * DocumentReader 工廠
 */
@Component
@Slf4j
public class DocumentReaderFactory {
    
    /**
     * 根據資源類型創建對應的 DocumentReader
     */
    public DocumentReader createReader(Resource resource) {
        String filename = resource.getFilename();
        if (filename == null) {
            throw new IllegalArgumentException("無法判斷文件類型");
        }
        
        String lowerFilename = filename.toLowerCase();
        
        // 根據文件擴展名選擇合適的 DocumentReader
        if (lowerFilename.endsWith(".pdf")) {
            return new PagePdfDocumentReader(resource);
        } else if (lowerFilename.endsWith(".txt") || lowerFilename.endsWith(".md")) {
            return new TextReader(resource);
        } else if (lowerFilename.endsWith(".html") || lowerFilename.endsWith(".htm")) {
            return new JsoupDocumentReader(resource);
        } else if (lowerFilename.endsWith(".json")) {
            return new JsonReader(resource);
        } else if (isTikaSupported(lowerFilename)) {
            return new TikaDocumentReader(resource);
        } else {
            throw new UnsupportedOperationException("不支援的文件類型: " + filename);
        }
    }
    
    private boolean isTikaSupported(String filename) {
        List<String> tikaSupported = Arrays.asList(
            ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx"
        );
        return tikaSupported.stream().anyMatch(filename::endsWith);
    }
}

/**
 * 壓縮檔案 ETL Pipeline 處理器
 */
@Component
@Slf4j
public class ArchiveETLProcessor {
    
    private final VectorStore vectorStore;
    private final DocumentReaderFactory documentReaderFactory;
    private final TokenTextSplitter textSplitter;
    
    public ArchiveETLProcessor(VectorStore vectorStore, DocumentReaderFactory documentReaderFactory) {
        this.vectorStore = vectorStore;
        this.documentReaderFactory = documentReaderFactory;
        this.textSplitter = new TokenTextSplitter(800, 200, 5, 1000, true);
    }
    
    /**
     * 完整的壓縮檔案 ETL 流程
     */
    public void processArchiveToVectorStore(Resource archiveResource) {
        try {
            // 1. Extract: 解壓縮並讀取內部文件
            ArchiveDocumentReader archiveReader = new ArchiveDocumentReader(archiveResource, documentReaderFactory);
            List<Document> documents = archiveReader.read();
            
            if (!documents.isEmpty()) {
                // 2. Transform: 文本分割和預處理
                List<Document> transformedDocuments = textSplitter.apply(documents);
                
                // 3. Load: 載入到向量資料庫
                vectorStore.write(transformedDocuments);
                
                log.info("成功處理壓縮檔案 {}，共產生 {} 個文檔片段", 
                    archiveResource.getFilename(), transformedDocuments.size());
            } else {
                log.warn("壓縮檔案 {} 中沒有找到支援的文件", archiveResource.getFilename());
            }
            
        } catch (Exception e) {
            log.error("壓縮檔案 ETL 處理失敗", e);
            throw new RuntimeException("壓縮檔案 ETL 處理失敗", e);
        }
    }
}
```

---

## 📝 本章重點回顧

1. **Spring AI TikaDocumentReader**：使用 Spring AI 官方推薦的 TikaDocumentReader 處理 Excel 和 PowerPoint 文件
2. **DocumentReader 實現**：遵循 Spring AI ETL Pipeline 架構，實現符合標準的文件讀取器
3. **OCR 技術整合**：建立自定義 OCR DocumentReader，整合到 Spring AI 生態系統
4. **壓縮檔案處理**：實現遞迴文件提取，支援壓縮檔案內的多種文件格式
5. **ETL Pipeline 整合**：所有處理器都整合到完整的 Extract-Transform-Load 流程中

### 依賴配置

#### Maven 依賴

```xml
<dependencies>
    <!-- Spring AI Tika DocumentReader -->
    <dependency>
        <groupId>org.springframework.ai</groupId>
        <artifactId>spring-ai-tika-document-reader</artifactId>
    </dependency>
    
    <!-- Spring AI PDF DocumentReader -->
    <dependency>
        <groupId>org.springframework.ai</groupId>
        <artifactId>spring-ai-pdf-document-reader</artifactId>
    </dependency>
    
    <!-- Tesseract OCR (需要額外安裝 Tesseract 軟體) -->
    <dependency>
        <groupId>net.sourceforge.tess4j</groupId>
        <artifactId>tess4j</artifactId>
        <version>5.7.0</version>
    </dependency>
</dependencies>
```

### 技術要點總結

| 技術點 | Spring AI 支援 | 實現方式 | 企業價值 |
|--------|---------------|----------|----------|
| **Excel 處理** | ✅ TikaDocumentReader | 官方支援 | 商業資料提取 |
| **PowerPoint 提取** | ✅ TikaDocumentReader | 官方支援 | 簡報內容分析 |
| **OCR 技術** | ❌ 需自定義 | 自定義 DocumentReader | 圖像文字識別 |
| **壓縮檔案處理** | ❌ 需自定義 | 自定義 DocumentReader | 批次檔案處理 |
| **ETL Pipeline** | ✅ 完整支援 | 官方架構 | 系統可維護性 |

### 最佳實踐建議

1. **使用官方 DocumentReader**：優先使用 Spring AI 官方提供的 DocumentReader
2. **遵循 ETL 架構**：所有自定義實現都應遵循 Spring AI ETL Pipeline 模式
3. **錯誤處理**：實現完善的錯誤處理，避免單一檔案錯誤影響整體處理
4. **記憶體管理**：處理大型檔案時注意記憶體使用
5. **並行處理**：利用 Stream API 的並行處理能力提高效率

### 下一步學習方向

在下一章中，我們將學習 ETL 的下篇 - 給向量資料加上 Buff，包括：
- 使用 Spring AI 的 DocumentTransformer 進行資料清理
- TokenTextSplitter 文本分塊最佳化
- KeywordMetadataEnricher 和 SummaryMetadataEnricher 元資料增強
- 向量品質提升和效能調優

---

**參考資料：**
- [Spring AI ETL Pipeline Documentation](https://docs.spring.io/spring-ai/reference/1.1-SNAPSHOT/api/etl-pipeline.html)
- [Spring AI TikaDocumentReader](https://docs.spring.io/spring-ai/reference/1.1-SNAPSHOT/api/etl-pipeline.html#_tika_docx_pptx_html)
- [Tesseract OCR](https://github.com/tesseract-ocr/tesseract)
- [Apache Tika Supported Formats](https://tika.apache.org/3.1.0/formats.html)