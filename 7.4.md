# 7.4 ETL (ä¸­) - æ“·å–é€²éšæ–‡ä»¶é¡å‹

> **æœ¬ç« é‡é»**ï¼šæ·±å…¥æ¢è¨é€²éšæ–‡ä»¶é¡å‹çš„æå–æŠ€è¡“ï¼ŒæŒæ¡ Excelã€PowerPointã€åœ–åƒã€å£“ç¸®æª”æ¡ˆç­‰è¤‡é›œæ ¼å¼çš„è™•ç†æ–¹æ³•ï¼Œå»ºç«‹å®Œæ•´çš„å¤šåª’é«”å…§å®¹æå–èƒ½åŠ›ã€‚

## ğŸ¯ å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬ç« å­¸ç¿’å¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š

- ğŸ¯ **è™•ç† Excel è©¦ç®—è¡¨**ï¼šæå–è¡¨æ ¼è³‡æ–™ã€åœ–è¡¨å’Œå…¬å¼å…§å®¹
- ğŸ¯ **è§£æ PowerPoint ç°¡å ±**ï¼šæå–æŠ•å½±ç‰‡æ–‡å­—ã€åœ–ç‰‡å’Œè¨»è§£
- ğŸ¯ **è™•ç†åœ–åƒæ–‡ä»¶**ï¼šä½¿ç”¨ OCR æŠ€è¡“æå–åœ–åƒä¸­çš„æ–‡å­—
- ğŸ¯ **è§£å£“ç¸®æª”æ¡ˆ**ï¼šè™•ç† ZIPã€RAR ç­‰å£“ç¸®æ ¼å¼çš„æ‰¹æ¬¡æå–
- ğŸ¯ **æ•´åˆå¤šåª’é«”å…§å®¹**ï¼šå»ºç«‹çµ±ä¸€çš„å¤šåª’é«”å…§å®¹è™•ç†ç®¡é“

---

## 7.4.1 Excel è©¦ç®—è¡¨æå–å™¨

### Excel æ–‡ä»¶ç‰¹é»èˆ‡æŒ‘æˆ°

**Excel æ–‡ä»¶çš„è¤‡é›œæ€§**ï¼š
- ğŸ“Š **å¤šå·¥ä½œè¡¨çµæ§‹**ï¼šä¸€å€‹æ–‡ä»¶å¯åŒ…å«å¤šå€‹å·¥ä½œè¡¨
- ğŸ“ˆ **è±å¯Œçš„è³‡æ–™é¡å‹**ï¼šæ•¸å€¼ã€æ–‡å­—ã€æ—¥æœŸã€å…¬å¼ã€åœ–è¡¨
- ğŸ¨ **æ ¼å¼åŒ–è³‡è¨Š**ï¼šå­—é«”ã€é¡è‰²ã€é‚Šæ¡†ã€åˆä½µå„²å­˜æ ¼
- ğŸ“‹ **å…ƒè³‡æ–™è±å¯Œ**ï¼šä½œè€…ã€å‰µå»ºæ™‚é–“ã€è¨»è§£ã€è¶…é€£çµ

### Excel æå–å™¨å¯¦ç¾

Spring AI ä½¿ç”¨ **TikaDocumentReader** ä¾†è™•ç† Excel æ–‡ä»¶ï¼Œé€™æ˜¯æ¨è–¦çš„æ–¹å¼ï¼š

#### ä¾è³´é…ç½®

```xml
<dependency>
    <groupId>org.springframework.ai</groupId>
    <artifactId>spring-ai-tika-document-reader</artifactId>
</dependency>
```

#### åŸºæœ¬ç”¨æ³•

```java
/**
 * Excel æ–‡ä»¶è®€å–å™¨
 */
@Component
@Slf4j
public class ExcelDocumentReader {
    
    /**
     * è®€å– Excel æ–‡ä»¶
     */
    public List<Document> readExcelFile(Resource resource) {
        log.info("Reading Excel file: {}", resource.getFilename());
        
        try {
            TikaDocumentReader tikaReader = new TikaDocumentReader(resource);
            List<Document> documents = tikaReader.read();
            
            // æ·»åŠ  Excel ç‰¹å®šçš„å…ƒè³‡æ–™
            documents.forEach(doc -> {
                doc.getMetadata().put("document_type", "EXCEL");
                doc.getMetadata().put("source_file", resource.getFilename());
                doc.getMetadata().put("extraction_method", "TIKA");
            });
            
            log.info("Successfully extracted {} documents from Excel file", documents.size());
            return documents;
            
        } catch (Exception e) {
            log.error("Failed to read Excel file: {}", resource.getFilename(), e);
            throw new RuntimeException("Excel æ–‡ä»¶è®€å–å¤±æ•—", e);
        }
    }
    
    /**
     * æ‰¹æ¬¡è™•ç†å¤šå€‹ Excel æ–‡ä»¶
     */
    public List<Document> readMultipleExcelFiles(List<Resource> resources) {
        List<Document> allDocuments = new ArrayList<>();
        
        for (Resource resource : resources) {
            try {
                List<Document> documents = readExcelFile(resource);
                allDocuments.addAll(documents);
            } catch (Exception e) {
                log.warn("Failed to process Excel file: {}", resource.getFilename(), e);
            }
        }
        
        return allDocuments;
    }
}

/**
 * ETL Pipeline æ•´åˆç¯„ä¾‹
 */
@Component
@Slf4j
public class ExcelETLProcessor {
    
    private final VectorStore vectorStore;
    private final TokenTextSplitter textSplitter;
    
    public ExcelETLProcessor(VectorStore vectorStore) {
        this.vectorStore = vectorStore;
        this.textSplitter = new TokenTextSplitter(500, 100, 5, 1000, true);
    }
    
    /**
     * å®Œæ•´çš„ Excel ETL æµç¨‹
     */
    public void processExcelToVectorStore(Resource excelResource) {
        try {
            // 1. Extract: è®€å– Excel æ–‡ä»¶
            TikaDocumentReader documentReader = new TikaDocumentReader(excelResource);
            List<Document> documents = documentReader.read();
            
            // 2. Transform: æ–‡æœ¬åˆ†å‰²å’Œé è™•ç†
            List<Document> transformedDocuments = textSplitter.apply(documents);
            
            // æ·»åŠ é¡å¤–çš„å…ƒè³‡æ–™å¢å¼·
            transformedDocuments.forEach(doc -> {
                doc.getMetadata().put("processing_timestamp", LocalDateTime.now().toString());
                doc.getMetadata().put("document_type", "EXCEL");
            });
            
            // 3. Load: è¼‰å…¥åˆ°å‘é‡è³‡æ–™åº«
            vectorStore.write(transformedDocuments);
            
            log.info("æˆåŠŸè™•ç† Excel æ–‡ä»¶ï¼Œå…±ç”¢ç”Ÿ {} å€‹æ–‡æª”ç‰‡æ®µ", transformedDocuments.size());
            
        } catch (Exception e) {
            log.error("Excel ETL è™•ç†å¤±æ•—", e);
            throw new RuntimeException("Excel ETL è™•ç†å¤±æ•—", e);
        }
    }
    
    /**
     * æ‰¹æ¬¡è™•ç†å¤šå€‹ Excel æ–‡ä»¶
     */
    public void processBatchExcelFiles(List<Resource> resources) {
        resources.parallelStream().forEach(resource -> {
            try {
                processExcelToVectorStore(resource);
            } catch (Exception e) {
                log.error("æ‰¹æ¬¡è™•ç†å¤±æ•—: {}", resource.getFilename(), e);
            }
        });
    }
}
```

---

## 7.4.2 PowerPoint ç°¡å ±æå–å™¨

### PowerPoint æå–å™¨å¯¦ç¾

Spring AI åŒæ¨£ä½¿ç”¨ **TikaDocumentReader** ä¾†è™•ç† PowerPoint æ–‡ä»¶ï¼š

```java
/**
 * PowerPoint ç°¡å ±è®€å–å™¨
 */
@Component
@Slf4j
public class PowerPointDocumentReader {
    
    /**
     * è®€å– PowerPoint æ–‡ä»¶
     */
    public List<Document> readPowerPointFile(Resource resource) {
        log.info("Reading PowerPoint file: {}", resource.getFilename());
        
        try {
            TikaDocumentReader tikaReader = new TikaDocumentReader(resource);
            List<Document> documents = tikaReader.read();
            
            // æ·»åŠ  PowerPoint ç‰¹å®šçš„å…ƒè³‡æ–™
            documents.forEach(doc -> {
                doc.getMetadata().put("document_type", "POWERPOINT");
                doc.getMetadata().put("source_file", resource.getFilename());
                doc.getMetadata().put("extraction_method", "TIKA");
            });
            
            log.info("Successfully extracted {} documents from PowerPoint file", documents.size());
            return documents;
            
        } catch (Exception e) {
            log.error("Failed to read PowerPoint file: {}", resource.getFilename(), e);
            throw new RuntimeException("PowerPoint æ–‡ä»¶è®€å–å¤±æ•—", e);
        }
    }
}

/**
 * PowerPoint ETL Pipeline æ•´åˆç¯„ä¾‹
 */
@Component
@Slf4j
public class PowerPointETLProcessor {
    
    private final VectorStore vectorStore;
    private final TokenTextSplitter textSplitter;
    
    public PowerPointETLProcessor(VectorStore vectorStore) {
        this.vectorStore = vectorStore;
        this.textSplitter = new TokenTextSplitter(800, 200, 5, 1000, true);
    }
    
    /**
     * å®Œæ•´çš„ PowerPoint ETL æµç¨‹
     */
    public void processPowerPointToVectorStore(Resource powerPointResource) {
        try {
            // 1. Extract: è®€å– PowerPoint æ–‡ä»¶
            TikaDocumentReader documentReader = new TikaDocumentReader(powerPointResource);
            List<Document> documents = documentReader.read();
            
            // 2. Transform: æ–‡æœ¬åˆ†å‰²å’Œé è™•ç†
            List<Document> transformedDocuments = textSplitter.apply(documents);
            
            // æ·»åŠ é¡å¤–çš„å…ƒè³‡æ–™å¢å¼·
            transformedDocuments.forEach(doc -> {
                doc.getMetadata().put("processing_timestamp", LocalDateTime.now().toString());
                doc.getMetadata().put("document_type", "POWERPOINT");
            });
            
            // 3. Load: è¼‰å…¥åˆ°å‘é‡è³‡æ–™åº«
            vectorStore.write(transformedDocuments);
            
            log.info("æˆåŠŸè™•ç† PowerPoint æ–‡ä»¶ï¼Œå…±ç”¢ç”Ÿ {} å€‹æ–‡æª”ç‰‡æ®µ", transformedDocuments.size());
            
        } catch (Exception e) {
            log.error("PowerPoint ETL è™•ç†å¤±æ•—", e);
            throw new RuntimeException("PowerPoint ETL è™•ç†å¤±æ•—", e);
        }
    }
    
    /**
     * æ‰¹æ¬¡è™•ç†å¤šå€‹ç°¡å ±æ–‡ä»¶
     */
    public void processBatchPowerPointFiles(List<Resource> resources) {
        resources.parallelStream().forEach(resource -> {
            try {
                processPowerPointToVectorStore(resource);
            } catch (Exception e) {
                log.error("æ‰¹æ¬¡è™•ç†å¤±æ•—: {}", resource.getFilename(), e);
            }
        });
    }
}
```

---

## 7.4.3 åœ–åƒæ–‡ä»¶ OCR æå–å™¨

### OCR DocumentReader å¯¦ç¾

Spring AI æ²’æœ‰å…§å»ºçš„ OCR DocumentReaderï¼Œä½†æˆ‘å€‘å¯ä»¥å¯¦ç¾ä¸€å€‹ç¬¦åˆ Spring AI æ¶æ§‹çš„ OCR DocumentReaderï¼š

```java
/**
 * OCR åœ–åƒæ–‡å­—æå–å™¨ (å¯¦ç¾ Spring AI DocumentReader æ¥å£)
 */
@Component
@Slf4j
public class ImageOCRDocumentReader implements DocumentReader {
    
    private final Resource resource;
    private final TesseractOCRService ocrService;
    
    public ImageOCRDocumentReader(Resource resource, TesseractOCRService ocrService) {
        this.resource = resource;
        this.ocrService = ocrService;
    }
    
    @Override
    public List<Document> read() {
        log.info("Extracting text from image: {}", resource.getFilename());
        
        try (InputStream inputStream = resource.getInputStream()) {
            
            // è®€å–åœ–åƒ
            BufferedImage image = ImageIO.read(inputStream);
            
            if (image != null) {
                // åœ–åƒé è™•ç†
                BufferedImage processedImage = preprocessImage(image);
                
                // OCR æ–‡å­—è­˜åˆ¥
                String extractedText = ocrService.extractText(processedImage);
                
                if (extractedText != null && !extractedText.trim().isEmpty()) {
                    Map<String, Object> metadata = createImageMetadata(image);
                    return List.of(new Document(extractedText.trim(), metadata));
                }
            }
            
            return List.of();
            
        } catch (Exception e) {
            log.error("Failed to extract text from image: {}", resource.getFilename(), e);
            throw new RuntimeException("OCR æ–‡å­—æå–å¤±æ•—", e);
        }
    }
    
    /**
     * åœ–åƒé è™•ç†
     */
    private BufferedImage preprocessImage(BufferedImage originalImage) {
        // è½‰æ›ç‚ºç°éš
        BufferedImage grayImage = new BufferedImage(
            originalImage.getWidth(), 
            originalImage.getHeight(), 
            BufferedImage.TYPE_BYTE_GRAY
        );
        
        Graphics2D g2d = grayImage.createGraphics();
        g2d.drawImage(originalImage, 0, 0, null);
        g2d.dispose();
        
        // å¢å¼·å°æ¯”åº¦
        return enhanceContrast(grayImage);
    }
    
    /**
     * å¢å¼·å°æ¯”åº¦
     */
    private BufferedImage enhanceContrast(BufferedImage image) {
        RescaleOp rescaleOp = new RescaleOp(1.2f, 15, null);
        return rescaleOp.filter(image, null);
    }
    
    private Map<String, Object> createImageMetadata(BufferedImage image) {
        Map<String, Object> metadata = new HashMap<>();
        metadata.put("document_type", "IMAGE_OCR");
        metadata.put("source_file", resource.getFilename());
        metadata.put("image_width", image.getWidth());
        metadata.put("image_height", image.getHeight());
        metadata.put("extraction_method", "TESSERACT_OCR");
        metadata.put("extraction_timestamp", LocalDateTime.now().toString());
        
        return metadata;
    }
}

/**
 * Tesseract OCR æœå‹™
 */
@Service
@Slf4j
public class TesseractOCRService {
    
    private final Tesseract tesseract;
    
    public TesseractOCRService() {
        this.tesseract = new Tesseract();
        configureTesseract();
    }
    
    private void configureTesseract() {
        try {
            // è¨­å®š Tesseract è³‡æ–™è·¯å¾‘ï¼ˆæ ¹æ“šä½œæ¥­ç³»çµ±èª¿æ•´ï¼‰
            String os = System.getProperty("os.name").toLowerCase();
            if (os.contains("win")) {
                tesseract.setDatapath("C:\\Program Files\\Tesseract-OCR\\tessdata");
            } else {
                tesseract.setDatapath("/usr/share/tesseract-ocr/4.00/tessdata");
            }
            
            // è¨­å®šèªè¨€ï¼ˆæ”¯æ´ç¹é«”ä¸­æ–‡å’Œè‹±æ–‡ï¼‰
            tesseract.setLanguage("chi_tra+eng");
            
            // è¨­å®š OCR å¼•æ“æ¨¡å¼
            tesseract.setOcrEngineMode(1); // LSTM OCR Engine
            
            // è¨­å®šé é¢åˆ†å‰²æ¨¡å¼
            tesseract.setPageSegMode(1); // Automatic page segmentation with OSD
            
        } catch (Exception e) {
            log.error("Failed to configure Tesseract", e);
        }
    }
    
    /**
     * å¾åœ–åƒæå–æ–‡å­—
     */
    public String extractText(BufferedImage image) {
        try {
            return tesseract.doOCR(image);
        } catch (Exception e) {
            log.error("OCR text extraction failed", e);
            return "";
        }
    }
}

/**
 * OCR ETL Pipeline è™•ç†å™¨
 */
@Component
@Slf4j
public class ImageOCRETLProcessor {
    
    private final VectorStore vectorStore;
    private final TesseractOCRService ocrService;
    private final TokenTextSplitter textSplitter;
    
    public ImageOCRETLProcessor(VectorStore vectorStore, TesseractOCRService ocrService) {
        this.vectorStore = vectorStore;
        this.ocrService = ocrService;
        this.textSplitter = new TokenTextSplitter(800, 200, 10, 1000, true);
    }
    
    /**
     * å®Œæ•´çš„åœ–åƒ OCR ETL æµç¨‹
     */
    public void processImageOCRToVectorStore(Resource imageResource) {
        try {
            // 1. Extract: ä½¿ç”¨ OCR è®€å–åœ–åƒæ–‡å­—
            ImageOCRDocumentReader documentReader = new ImageOCRDocumentReader(imageResource, ocrService);
            List<Document> documents = documentReader.read();
            
            if (!documents.isEmpty()) {
                // 2. Transform: æ–‡æœ¬åˆ†å‰²ï¼ˆOCR æ–‡å­—é€šå¸¸è¼ƒçŸ­ï¼Œå¯èƒ½ä¸éœ€è¦åˆ†å‰²ï¼‰
                List<Document> transformedDocuments = documents.stream()
                    .filter(doc -> doc.getContent().trim().length() > 10) // éæ¿¾å¤ªçŸ­çš„æ–‡å­—
                    .collect(Collectors.toList());
                
                // å¦‚æœæ–‡å­—è¼ƒé•·ï¼Œé€²è¡Œåˆ†å‰²
                if (transformedDocuments.stream().anyMatch(doc -> doc.getContent().length() > 500)) {
                    transformedDocuments = textSplitter.apply(transformedDocuments);
                }
                
                // 3. Load: è¼‰å…¥åˆ°å‘é‡è³‡æ–™åº«
                vectorStore.write(transformedDocuments);
                
                log.info("æˆåŠŸè™•ç†åœ–åƒ OCRï¼Œå…±ç”¢ç”Ÿ {} å€‹æ–‡æª”ç‰‡æ®µ", transformedDocuments.size());
            } else {
                log.warn("åœ–åƒ {} æœªæå–åˆ°ä»»ä½•æ–‡å­—", imageResource.getFilename());
            }
            
        } catch (Exception e) {
            log.error("åœ–åƒ OCR ETL è™•ç†å¤±æ•—", e);
            throw new RuntimeException("åœ–åƒ OCR ETL è™•ç†å¤±æ•—", e);
        }
    }
    
    /**
     * æ‰¹æ¬¡è™•ç†å¤šå€‹åœ–åƒæ–‡ä»¶
     */
    public void processBatchImageFiles(List<Resource> imageResources) {
        imageResources.parallelStream().forEach(resource -> {
            try {
                processImageOCRToVectorStore(resource);
            } catch (Exception e) {
                log.error("æ‰¹æ¬¡ OCR è™•ç†å¤±æ•—: {}", resource.getFilename(), e);
            }
        });
    }
}
```

---

## 7.4.4 å£“ç¸®æª”æ¡ˆè™•ç†å™¨

### å£“ç¸®æª”æ¡ˆ DocumentReader å¯¦ç¾

Spring AI æ²’æœ‰å…§å»ºçš„å£“ç¸®æª”æ¡ˆè™•ç†å™¨ï¼Œä½†æˆ‘å€‘å¯ä»¥å¯¦ç¾ä¸€å€‹ç¬¦åˆ Spring AI æ¶æ§‹çš„å£“ç¸®æª”æ¡ˆè™•ç†å™¨ï¼š

```java
/**
 * å£“ç¸®æª”æ¡ˆ DocumentReader (å¯¦ç¾ Spring AI DocumentReader æ¥å£)
 */
@Component
@Slf4j
public class ArchiveDocumentReader implements DocumentReader {
    
    private final Resource archiveResource;
    private final DocumentReaderFactory documentReaderFactory;
    
    public ArchiveDocumentReader(Resource archiveResource, DocumentReaderFactory documentReaderFactory) {
        this.archiveResource = archiveResource;
        this.documentReaderFactory = documentReaderFactory;
    }
    
    @Override
    public List<Document> read() {
        log.info("Extracting archive file: {}", archiveResource.getFilename());
        
        String fileName = archiveResource.getFilename().toLowerCase();
        
        try {
            if (fileName.endsWith(".zip")) {
                return extractZipFile();
            } else {
                log.warn("Unsupported archive format: {}", fileName);
                return List.of();
            }
        } catch (Exception e) {
            log.error("Failed to extract archive: {}", archiveResource.getFilename(), e);
            throw new RuntimeException("å£“ç¸®æª”æ¡ˆè™•ç†å¤±æ•—", e);
        }
    }
    
    /**
     * æå– ZIP æª”æ¡ˆ
     */
    private List<Document> extractZipFile() throws IOException {
        List<Document> documents = new ArrayList<>();
        
        try (InputStream inputStream = archiveResource.getInputStream();
             ZipInputStream zipInputStream = new ZipInputStream(inputStream)) {
            
            ZipEntry entry;
            while ((entry = zipInputStream.getNextEntry()) != null) {
                
                if (!entry.isDirectory() && isSupportedFileType(entry.getName())) {
                    try {
                        List<Document> entryDocuments = extractZipEntry(zipInputStream, entry);
                        documents.addAll(entryDocuments);
                    } catch (Exception e) {
                        log.warn("Failed to extract zip entry {}: {}", entry.getName(), e.getMessage());
                    }
                }
                
                zipInputStream.closeEntry();
            }
        }
        
        return documents;
    }
    
    /**
     * æå– ZIP æ¢ç›®
     */
    private List<Document> extractZipEntry(ZipInputStream zipInputStream, ZipEntry entry) throws IOException {
        String entryName = entry.getName();
        
        // è®€å–æ¢ç›®å…§å®¹åˆ°è¨˜æ†¶é«”
        ByteArrayOutputStream baos = new ByteArrayOutputStream();
        byte[] buffer = new byte[8192];
        int bytesRead;
        
        while ((bytesRead = zipInputStream.read(buffer)) != -1) {
            baos.write(buffer, 0, bytesRead);
        }
        
        byte[] entryData = baos.toByteArray();
        
        // å»ºç«‹è¨˜æ†¶é«”è³‡æº
        Resource entryResource = new ByteArrayResource(entryData, entryName);
        
        try {
            // æ ¹æ“šæ–‡ä»¶é¡å‹é¸æ“‡é©ç•¶çš„ DocumentReader
            DocumentReader reader = documentReaderFactory.createReader(entryResource);
            List<Document> documents = reader.read();
            
            // æ·»åŠ å£“ç¸®æª”æ¡ˆç›¸é—œçš„å…ƒè³‡æ–™
            documents.forEach(doc -> {
                doc.getMetadata().put("archive_source", archiveResource.getFilename());
                doc.getMetadata().put("archive_entry", entryName);
                doc.getMetadata().put("extraction_method", "ZIP_ARCHIVE");
            });
            
            return documents;
            
        } catch (Exception e) {
            log.warn("No suitable reader found for archive entry: {}", entryName);
            return List.of();
        }
    }
    
    /**
     * æª¢æŸ¥æª”æ¡ˆé¡å‹æ˜¯å¦æ”¯æ´
     */
    private boolean isSupportedFileType(String fileName) {
        String lowerFileName = fileName.toLowerCase();
        
        // æ”¯æ´çš„æª”æ¡ˆé¡å‹
        List<String> supportedTypes = Arrays.asList(
            ".pdf", ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx",
            ".txt", ".md", ".html", ".htm", ".xml", ".json"
        );
        
        return supportedTypes.stream().anyMatch(lowerFileName::endsWith);
    }
}

/**
 * DocumentReader å·¥å» 
 */
@Component
@Slf4j
public class DocumentReaderFactory {
    
    /**
     * æ ¹æ“šè³‡æºé¡å‹å‰µå»ºå°æ‡‰çš„ DocumentReader
     */
    public DocumentReader createReader(Resource resource) {
        String filename = resource.getFilename();
        if (filename == null) {
            throw new IllegalArgumentException("ç„¡æ³•åˆ¤æ–·æ–‡ä»¶é¡å‹");
        }
        
        String lowerFilename = filename.toLowerCase();
        
        // æ ¹æ“šæ–‡ä»¶æ“´å±•åé¸æ“‡åˆé©çš„ DocumentReader
        if (lowerFilename.endsWith(".pdf")) {
            return new PagePdfDocumentReader(resource);
        } else if (lowerFilename.endsWith(".txt") || lowerFilename.endsWith(".md")) {
            return new TextReader(resource);
        } else if (lowerFilename.endsWith(".html") || lowerFilename.endsWith(".htm")) {
            return new JsoupDocumentReader(resource);
        } else if (lowerFilename.endsWith(".json")) {
            return new JsonReader(resource);
        } else if (isTikaSupported(lowerFilename)) {
            return new TikaDocumentReader(resource);
        } else {
            throw new UnsupportedOperationException("ä¸æ”¯æ´çš„æ–‡ä»¶é¡å‹: " + filename);
        }
    }
    
    private boolean isTikaSupported(String filename) {
        List<String> tikaSupported = Arrays.asList(
            ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx"
        );
        return tikaSupported.stream().anyMatch(filename::endsWith);
    }
}

/**
 * å£“ç¸®æª”æ¡ˆ ETL Pipeline è™•ç†å™¨
 */
@Component
@Slf4j
public class ArchiveETLProcessor {
    
    private final VectorStore vectorStore;
    private final DocumentReaderFactory documentReaderFactory;
    private final TokenTextSplitter textSplitter;
    
    public ArchiveETLProcessor(VectorStore vectorStore, DocumentReaderFactory documentReaderFactory) {
        this.vectorStore = vectorStore;
        this.documentReaderFactory = documentReaderFactory;
        this.textSplitter = new TokenTextSplitter(800, 200, 5, 1000, true);
    }
    
    /**
     * å®Œæ•´çš„å£“ç¸®æª”æ¡ˆ ETL æµç¨‹
     */
    public void processArchiveToVectorStore(Resource archiveResource) {
        try {
            // 1. Extract: è§£å£“ç¸®ä¸¦è®€å–å…§éƒ¨æ–‡ä»¶
            ArchiveDocumentReader archiveReader = new ArchiveDocumentReader(archiveResource, documentReaderFactory);
            List<Document> documents = archiveReader.read();
            
            if (!documents.isEmpty()) {
                // 2. Transform: æ–‡æœ¬åˆ†å‰²å’Œé è™•ç†
                List<Document> transformedDocuments = textSplitter.apply(documents);
                
                // 3. Load: è¼‰å…¥åˆ°å‘é‡è³‡æ–™åº«
                vectorStore.write(transformedDocuments);
                
                log.info("æˆåŠŸè™•ç†å£“ç¸®æª”æ¡ˆ {}ï¼Œå…±ç”¢ç”Ÿ {} å€‹æ–‡æª”ç‰‡æ®µ", 
                    archiveResource.getFilename(), transformedDocuments.size());
            } else {
                log.warn("å£“ç¸®æª”æ¡ˆ {} ä¸­æ²’æœ‰æ‰¾åˆ°æ”¯æ´çš„æ–‡ä»¶", archiveResource.getFilename());
            }
            
        } catch (Exception e) {
            log.error("å£“ç¸®æª”æ¡ˆ ETL è™•ç†å¤±æ•—", e);
            throw new RuntimeException("å£“ç¸®æª”æ¡ˆ ETL è™•ç†å¤±æ•—", e);
        }
    }
}
```

---

## ğŸ“ æœ¬ç« é‡é»å›é¡§

1. **Spring AI TikaDocumentReader**ï¼šä½¿ç”¨ Spring AI å®˜æ–¹æ¨è–¦çš„ TikaDocumentReader è™•ç† Excel å’Œ PowerPoint æ–‡ä»¶
2. **DocumentReader å¯¦ç¾**ï¼šéµå¾ª Spring AI ETL Pipeline æ¶æ§‹ï¼Œå¯¦ç¾ç¬¦åˆæ¨™æº–çš„æ–‡ä»¶è®€å–å™¨
3. **OCR æŠ€è¡“æ•´åˆ**ï¼šå»ºç«‹è‡ªå®šç¾© OCR DocumentReaderï¼Œæ•´åˆåˆ° Spring AI ç”Ÿæ…‹ç³»çµ±
4. **å£“ç¸®æª”æ¡ˆè™•ç†**ï¼šå¯¦ç¾éè¿´æ–‡ä»¶æå–ï¼Œæ”¯æ´å£“ç¸®æª”æ¡ˆå…§çš„å¤šç¨®æ–‡ä»¶æ ¼å¼
5. **ETL Pipeline æ•´åˆ**ï¼šæ‰€æœ‰è™•ç†å™¨éƒ½æ•´åˆåˆ°å®Œæ•´çš„ Extract-Transform-Load æµç¨‹ä¸­

### ä¾è³´é…ç½®

#### Maven ä¾è³´

```xml
<dependencies>
    <!-- Spring AI Tika DocumentReader -->
    <dependency>
        <groupId>org.springframework.ai</groupId>
        <artifactId>spring-ai-tika-document-reader</artifactId>
    </dependency>
    
    <!-- Spring AI PDF DocumentReader -->
    <dependency>
        <groupId>org.springframework.ai</groupId>
        <artifactId>spring-ai-pdf-document-reader</artifactId>
    </dependency>
    
    <!-- Tesseract OCR (éœ€è¦é¡å¤–å®‰è£ Tesseract è»Ÿé«”) -->
    <dependency>
        <groupId>net.sourceforge.tess4j</groupId>
        <artifactId>tess4j</artifactId>
        <version>5.7.0</version>
    </dependency>
</dependencies>
```

### æŠ€è¡“è¦é»ç¸½çµ

| æŠ€è¡“é» | Spring AI æ”¯æ´ | å¯¦ç¾æ–¹å¼ | ä¼æ¥­åƒ¹å€¼ |
|--------|---------------|----------|----------|
| **Excel è™•ç†** | âœ… TikaDocumentReader | å®˜æ–¹æ”¯æ´ | å•†æ¥­è³‡æ–™æå– |
| **PowerPoint æå–** | âœ… TikaDocumentReader | å®˜æ–¹æ”¯æ´ | ç°¡å ±å…§å®¹åˆ†æ |
| **OCR æŠ€è¡“** | âŒ éœ€è‡ªå®šç¾© | è‡ªå®šç¾© DocumentReader | åœ–åƒæ–‡å­—è­˜åˆ¥ |
| **å£“ç¸®æª”æ¡ˆè™•ç†** | âŒ éœ€è‡ªå®šç¾© | è‡ªå®šç¾© DocumentReader | æ‰¹æ¬¡æª”æ¡ˆè™•ç† |
| **ETL Pipeline** | âœ… å®Œæ•´æ”¯æ´ | å®˜æ–¹æ¶æ§‹ | ç³»çµ±å¯ç¶­è­·æ€§ |

### æœ€ä½³å¯¦è¸å»ºè­°

1. **ä½¿ç”¨å®˜æ–¹ DocumentReader**ï¼šå„ªå…ˆä½¿ç”¨ Spring AI å®˜æ–¹æä¾›çš„ DocumentReader
2. **éµå¾ª ETL æ¶æ§‹**ï¼šæ‰€æœ‰è‡ªå®šç¾©å¯¦ç¾éƒ½æ‡‰éµå¾ª Spring AI ETL Pipeline æ¨¡å¼
3. **éŒ¯èª¤è™•ç†**ï¼šå¯¦ç¾å®Œå–„çš„éŒ¯èª¤è™•ç†ï¼Œé¿å…å–®ä¸€æª”æ¡ˆéŒ¯èª¤å½±éŸ¿æ•´é«”è™•ç†
4. **è¨˜æ†¶é«”ç®¡ç†**ï¼šè™•ç†å¤§å‹æª”æ¡ˆæ™‚æ³¨æ„è¨˜æ†¶é«”ä½¿ç”¨
5. **ä¸¦è¡Œè™•ç†**ï¼šåˆ©ç”¨ Stream API çš„ä¸¦è¡Œè™•ç†èƒ½åŠ›æé«˜æ•ˆç‡

### ä¸‹ä¸€æ­¥å­¸ç¿’æ–¹å‘

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘å€‘å°‡å­¸ç¿’ ETL çš„ä¸‹ç¯‡ - çµ¦å‘é‡è³‡æ–™åŠ ä¸Š Buffï¼ŒåŒ…æ‹¬ï¼š
- ä½¿ç”¨ Spring AI çš„ DocumentTransformer é€²è¡Œè³‡æ–™æ¸…ç†
- TokenTextSplitter æ–‡æœ¬åˆ†å¡Šæœ€ä½³åŒ–
- KeywordMetadataEnricher å’Œ SummaryMetadataEnricher å…ƒè³‡æ–™å¢å¼·
- å‘é‡å“è³ªæå‡å’Œæ•ˆèƒ½èª¿å„ª

---

**åƒè€ƒè³‡æ–™ï¼š**
- [Spring AI ETL Pipeline Documentation](https://docs.spring.io/spring-ai/reference/1.1-SNAPSHOT/api/etl-pipeline.html)
- [Spring AI TikaDocumentReader](https://docs.spring.io/spring-ai/reference/1.1-SNAPSHOT/api/etl-pipeline.html#_tika_docx_pptx_html)
- [Tesseract OCR](https://github.com/tesseract-ocr/tesseract)
- [Apache Tika Supported Formats](https://tika.apache.org/3.1.0/formats.html)