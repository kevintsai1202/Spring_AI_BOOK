# 7.5 ETL (下) - 給向量資料加上 Buff

> **本章重點**：深入探討 ETL 流程中的資料轉換（Transform）技術，掌握文本清理、智能分塊、元資料增強和向量品質優化，讓 RAG 系統的知識庫更加強大和精準。

## 🎯 學習目標

完成本章學習後，您將能夠：

- 🎯 **掌握文本清理技術**：實現智能的文本預處理和標準化
- 🎯 **優化文本分塊策略**：設計適合不同文檔類型的分塊算法
- 🎯 **增強元資料管理**：建立豐富的元資料標記和分類系統
- 🎯 **提升向量品質**：實現向量品質評估和優化機制
- 🎯 **建立完整 ETL 管道**：整合提取、轉換、載入的完整流程

---

## 7.5.1 智能文本清理與預處理

### 文本清理的重要性

**為什麼需要文本清理？**
- 📝 **提高向量品質**：清理後的文本生成更準確的向量表示
- 🎯 **增強檢索精度**：減少噪音提高相似性搜尋的準確性
- 💾 **節省存儲空間**：移除冗餘內容減少存儲需求
- ⚡ **提升處理效率**：標準化格式加快處理速度

### 文本清理服務實現

```java
/**
 * 智能文本清理服務
 */
@Service
@Slf4j
public class TextCleaningService {
    
    private static final Pattern EMAIL_PATTERN = 
        Pattern.compile("[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}");
    
    private static final Pattern URL_PATTERN = 
        Pattern.compile("https?://[\\w\\-._~:/?#\\[\\]@!$&'()*+,;=%]+");
    
    private static final Pattern PHONE_PATTERN = 
        Pattern.compile("\\b\\d{2,4}[-\\s]?\\d{3,4}[-\\s]?\\d{3,4}\\b");
    
    /**
     * 綜合文本清理
     */
    public String cleanText(String rawText, TextCleaningConfig config) {
        if (rawText == null || rawText.trim().isEmpty()) {
            return "";
        }
        
        String cleanedText = rawText;
        
        // 1. 基礎清理
        if (config.isRemoveExtraWhitespace()) {
            cleanedText = removeExtraWhitespace(cleanedText);
        }
        
        // 2. 移除特殊字符
        if (config.isRemoveSpecialCharacters()) {
            cleanedText = removeSpecialCharacters(cleanedText);
        }
        
        // 3. 標準化換行符
        if (config.isNormalizeLineBreaks()) {
            cleanedText = normalizeLineBreaks(cleanedText);
        }
        
        // 4. 移除敏感資訊
        if (config.isRemoveSensitiveInfo()) {
            cleanedText = removeSensitiveInformation(cleanedText);
        }
        
        // 5. 語言特定清理
        if (config.getLanguage() != null) {
            cleanedText = applyLanguageSpecificCleaning(cleanedText, config.getLanguage());
        }
        
        // 6. 自定義清理規則
        if (config.getCustomRules() != null && !config.getCustomRules().isEmpty()) {
            cleanedText = applyCustomRules(cleanedText, config.getCustomRules());
        }
        
        return cleanedText.trim();
    }
    
    /**
     * 移除多餘空白字符
     */
    private String removeExtraWhitespace(String text) {
        return text
            .replaceAll("\\s+", " ")  // 多個空白字符合併為一個
            .replaceAll("\\n\\s*\\n", "\\n\\n")  // 多個空行合併為兩個
            .trim();
    }
    
    /**
     * 移除特殊字符
     */
    private String removeSpecialCharacters(String text) {
        // 保留基本標點符號，移除其他特殊字符
        return text.replaceAll("[^\\p{L}\\p{N}\\p{P}\\p{Z}\\r\\n]", "");
    }
    
    /**
     * 標準化換行符
     */
    private String normalizeLineBreaks(String text) {
        return text
            .replaceAll("\\r\\n", "\\n")  // Windows 換行符
            .replaceAll("\\r", "\\n");    // Mac 換行符
    }
    
    /**
     * 移除敏感資訊
     */
    private String removeSensitiveInformation(String text) {
        String result = text;
        
        // 移除電子郵件
        result = EMAIL_PATTERN.matcher(result).replaceAll("[EMAIL]");
        
        // 移除 URL
        result = URL_PATTERN.matcher(result).replaceAll("[URL]");
        
        // 移除電話號碼
        result = PHONE_PATTERN.matcher(result).replaceAll("[PHONE]");
        
        // 移除身分證號碼（台灣格式）
        result = result.replaceAll("\\b[A-Z]\\d{9}\\b", "[ID]");
        
        // 移除信用卡號碼
        result = result.replaceAll("\\b\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\\b", "[CARD]");
        
        return result;
    }
    
    /**
     * 語言特定清理
     */
    private String applyLanguageSpecificCleaning(String text, String language) {
        switch (language.toLowerCase()) {
            case "zh":
            case "zh-tw":
            case "zh-cn":
                return cleanChineseText(text);
            case "en":
                return cleanEnglishText(text);
            case "ja":
                return cleanJapaneseText(text);
            default:
                return text;
        }
    }
    
    /**
     * 中文文本清理
     */
    private String cleanChineseText(String text) {
        return text
            // 統一中文標點符號
            .replaceAll("，", "，")
            .replaceAll("。", "。")
            .replaceAll("！", "！")
            .replaceAll("？", "？")
            // 移除全形空格
            .replaceAll("　", " ")
            // 統一引號
            .replaceAll("[""]", "\"")
            .replaceAll("['']", "'");
    }
    
    /**
     * 英文文本清理
     */
    private String cleanEnglishText(String text) {
        return text
            // 統一引號
            .replaceAll("[""]", "\"")
            .replaceAll("['']", "'")
            // 移除多餘空格
            .replaceAll("\\s+", " ");
    }
    
    /**
     * 日文文本清理
     */
    private String cleanJapaneseText(String text) {
        return text
            // 統一日文標點符號
            .replaceAll("、", "、")
            .replaceAll("。", "。")
            // 移除全形空格
            .replaceAll("　", " ");
    }
    
    /**
     * 應用自定義清理規則
     */
    private String applyCustomRules(String text, List<TextCleaningRule> customRules) {
        String result = text;
        for (TextCleaningRule rule : customRules) {
            result = rule.apply(result);
        }
        return result;
    }
}
```

### 文本清理配置類

```java
/**
 * 文本清理配置
 */
@Data
@Builder
@AllArgsConstructor
@NoArgsConstructor
public class TextCleaningConfig {
    
    @Builder.Default
    private boolean removeExtraWhitespace = true;
    
    @Builder.Default
    private boolean removeSpecialCharacters = false;
    
    @Builder.Default
    private boolean normalizeLineBreaks = true;
    
    @Builder.Default
    private boolean removeSensitiveInfo = true;
    
    private String language;
    
    private List<TextCleaningRule> customRules;
}

/**
 * 自定義清理規則接口
 */
@FunctionalInterface
public interface TextCleaningRule {
    String apply(String text);
}
```

---

## 7.5.2 智能文本分塊策略

### 為什麼需要智能分塊？

**分塊的重要性**：
- 🎯 **適應模型限制**：符合 LLM 的 token 限制要求
- 📝 **保持語義完整**：避免在語義邊界中間分割
- 🔍 **提高檢索精度**：合適大小的塊提供更精確的相似性匹配
- ⚡ **優化處理效率**：平衡塊大小與處理速度

### Spring AI 標準文本分塊實現

```java
/**
 * 智能文本分塊服務
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class DocumentChunkingService {
    
    private final TextCleaningService textCleaningService;
    
    /**
     * 使用 Spring AI TokenTextSplitter 進行分塊
     */
    public List<Document> chunkDocuments(List<Document> documents, ChunkingConfig config) {
        log.info("Chunking {} documents with config: {}", documents.size(), config);
        
        // 1. 清理文本
        List<Document> cleanedDocuments = documents.stream()
            .map(doc -> cleanDocument(doc, config.getCleaningConfig()))
            .collect(Collectors.toList());
        
        // 2. 創建 TokenTextSplitter
        TokenTextSplitter splitter = createTokenTextSplitter(config);
        
        // 3. 執行分塊
        List<Document> chunkedDocuments = splitter.apply(cleanedDocuments);
        
        log.info("Successfully chunked into {} documents", chunkedDocuments.size());
        
        return chunkedDocuments;
    }
    
    /**
     * 創建 TokenTextSplitter 實例
     */
    private TokenTextSplitter createTokenTextSplitter(ChunkingConfig config) {
        return new TokenTextSplitter(
            config.getDefaultChunkSize(),
            config.getMinChunkSizeChars(),
            config.getMinChunkLengthToEmbed(),
            config.getMaxNumChunks(),
            config.isKeepSeparator()
        );
    }
    
    /**
     * 清理單個文檔
     */
    private Document cleanDocument(Document document, TextCleaningConfig cleaningConfig) {
        if (cleaningConfig == null) {
            return document;
        }
        
        String cleanedContent = textCleaningService.cleanText(
            document.getContent(), 
            cleaningConfig
        );
        
        // 創建新的文檔，保留原有元資料
        return Document.builder()
            .content(cleanedContent)
            .metadata(new HashMap<>(document.getMetadata()))
            .build();
    }
    
    /**
     * 自定義語義分塊（基於段落和句子）
     */
    public List<Document> semanticChunking(List<Document> documents, SemanticChunkingConfig config) {
        List<Document> result = new ArrayList<>();
        
        for (Document document : documents) {
            List<Document> documentChunks = semanticChunkDocument(document, config);
            result.addAll(documentChunks);
        }
        
        return result;
    }
    
    /**
     * 對單個文檔進行語義分塊
     */
    private List<Document> semanticChunkDocument(Document document, SemanticChunkingConfig config) {
        String content = document.getContent();
        List<Document> chunks = new ArrayList<>();
        
        // 1. 先按段落分割
        String[] paragraphs = content.split("\\n\\s*\\n");
        
        StringBuilder currentChunk = new StringBuilder();
        int currentTokenCount = 0;
        int chunkIndex = 0;
        
        for (String paragraph : paragraphs) {
            paragraph = paragraph.trim();
            if (paragraph.isEmpty()) {
                continue;
            }
            
            // 估算 token 數量（簡化計算）
            int paragraphTokens = estimateTokenCount(paragraph);
            
            // 如果加入此段落會超過限制，先創建一個塊
            if (currentTokenCount + paragraphTokens > config.getMaxTokensPerChunk() 
                && currentChunk.length() > 0) {
                
                Document chunk = createChunk(
                    document, 
                    currentChunk.toString(), 
                    chunkIndex++, 
                    config
                );
                chunks.add(chunk);
                
                currentChunk = new StringBuilder();
                currentTokenCount = 0;
            }
            
            // 加入段落
            if (currentChunk.length() > 0) {
                currentChunk.append("\\n\\n");
            }
            currentChunk.append(paragraph);
            currentTokenCount += paragraphTokens;
        }
        
        // 處理最後一個塊
        if (currentChunk.length() > 0) {
            Document chunk = createChunk(
                document, 
                currentChunk.toString(), 
                chunkIndex, 
                config
            );
            chunks.add(chunk);
        }
        
        return chunks;
    }
    
    /**
     * 創建文檔塊
     */
    private Document createChunk(Document originalDoc, String content, int chunkIndex, SemanticChunkingConfig config) {
        Map<String, Object> metadata = new HashMap<>(originalDoc.getMetadata());
        
        // 添加分塊元資料
        metadata.put("chunk_index", chunkIndex);
        metadata.put("chunk_method", "semantic");
        metadata.put("original_document_id", originalDoc.getId());
        metadata.put("chunk_token_count", estimateTokenCount(content));
        
        // 如果配置了重疊，添加重疊資訊
        if (config.getOverlapTokens() > 0) {
            metadata.put("overlap_tokens", config.getOverlapTokens());
        }
        
        return Document.builder()
            .content(content)
            .metadata(metadata)
            .build();
    }
    
    /**
     * 估算文本的 token 數量（簡化實現）
     */
    private int estimateTokenCount(String text) {
        // 簡化估算：平均每個字符約 0.25 tokens
        return (int) Math.ceil(text.length() * 0.25);
    }
}
```

### 分塊配置類

```java
/**
 * 分塊配置
 */
@Data
@Builder
@AllArgsConstructor
@NoArgsConstructor
public class ChunkingConfig {
    
    /**
     * 默認塊大小（tokens）
     */
    @Builder.Default
    private int defaultChunkSize = 1000;
    
    /**
     * 最小塊大小（字符）
     */
    @Builder.Default
    private int minChunkSizeChars = 350;
    
    /**
     * 最小塊長度以進行嵌入
     */
    @Builder.Default
    private int minChunkLengthToEmbed = 10;
    
    /**
     * 最大塊數量
     */
    @Builder.Default
    private int maxNumChunks = 10000;
    
    /**
     * 是否保留分隔符
     */
    @Builder.Default
    private boolean keepSeparator = true;
    
    /**
     * 文本清理配置
     */
    private TextCleaningConfig cleaningConfig;
}

/**
 * 語義分塊配置
 */
@Data
@Builder
@AllArgsConstructor
@NoArgsConstructor
public class SemanticChunkingConfig {
    
    /**
     * 每個塊的最大 token 數
     */
    @Builder.Default
    private int maxTokensPerChunk = 1000;
    
    /**
     * 塊之間的重疊 token 數
     */
    @Builder.Default
    private int overlapTokens = 100;
    
    /**
     * 是否保持段落完整性
     */
    @Builder.Default
    private boolean preserveParagraphs = true;
    
    /**
     * 是否保持句子完整性
     */
    @Builder.Default
    private boolean preserveSentences = true;
}
```

---

## 7.5.3 元資料增強與管理

### 元資料的重要性

**為什麼需要豐富的元資料？**
- 🎯 **提高檢索精度**：基於元資料過濾和排序
- 📊 **支援分析統計**：追蹤文檔來源、類型、處理時間
- 🔍 **增強搜索功能**：多維度搜索和過濾
- 📝 **審計追蹤**：記錄數據處理歷程

### 元資料增強服務

```java
/**
 * 元資料增強服務
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class MetadataEnrichmentService {
    
    private final ChatModel chatModel;
    private final LanguageDetectionService languageDetector;
    
    /**
     * 使用 Spring AI KeywordMetadataEnricher 增強關鍵詞
     */
    public List<Document> enrichWithKeywords(List<Document> documents, int keywordCount) {
        KeywordMetadataEnricher enricher = new KeywordMetadataEnricher(chatModel, keywordCount);
        return enricher.apply(documents);
    }
    
    /**
     * 使用 Spring AI SummaryMetadataEnricher 增強摘要
     */
    public List<Document> enrichWithSummaries(List<Document> documents, List<SummaryType> summaryTypes) {
        SummaryMetadataEnricher enricher = new SummaryMetadataEnricher(chatModel, summaryTypes);
        return enricher.apply(documents);
    }
    
    /**
     * 綜合元資料增強
     */
    public List<Document> enrichMetadata(List<Document> documents, MetadataEnrichmentConfig config) {
        List<Document> enrichedDocuments = new ArrayList<>(documents);
        
        // 1. 基礎元資料增強
        if (config.isEnableBasicMetadata()) {
            enrichedDocuments = enrichBasicMetadata(enrichedDocuments);
        }
        
        // 2. 語言檢測
        if (config.isEnableLanguageDetection()) {
            enrichedDocuments = enrichLanguageMetadata(enrichedDocuments);
        }
        
        // 3. 內容統計
        if (config.isEnableContentStatistics()) {
            enrichedDocuments = enrichContentStatistics(enrichedDocuments);
        }
        
        // 4. 關鍵詞提取（使用 AI）
        if (config.isEnableKeywordExtraction() && config.getKeywordCount() > 0) {
            enrichedDocuments = enrichWithKeywords(enrichedDocuments, config.getKeywordCount());
        }
        
        // 5. 摘要生成（使用 AI）
        if (config.isEnableSummaryGeneration() && config.getSummaryTypes() != null) {
            enrichedDocuments = enrichWithSummaries(enrichedDocuments, config.getSummaryTypes());
        }
        
        // 6. 自定義分類
        if (config.isEnableCustomClassification()) {
            enrichedDocuments = enrichCustomClassification(enrichedDocuments, config);
        }
        
        return enrichedDocuments;
    }
    
    /**
     * 基礎元資料增強
     */
    private List<Document> enrichBasicMetadata(List<Document> documents) {
        return documents.stream()
            .map(this::enrichBasicMetadataForDocument)
            .collect(Collectors.toList());
    }
    
    private Document enrichBasicMetadataForDocument(Document document) {
        Map<String, Object> metadata = new HashMap<>(document.getMetadata());
        
        // 處理時間戳
        metadata.put("processed_at", LocalDateTime.now().toString());
        
        // 內容哈希（用於去重）
        metadata.put("content_hash", calculateContentHash(document.getContent()));
        
        // 文檔 ID（如果沒有）
        if (!metadata.containsKey("id")) {
            metadata.put("id", UUID.randomUUID().toString());
        }
        
        return Document.builder()
            .content(document.getContent())
            .metadata(metadata)
            .build();
    }
    
    /**
     * 語言檢測增強
     */
    private List<Document> enrichLanguageMetadata(List<Document> documents) {
        return documents.stream()
            .map(this::detectLanguageForDocument)
            .collect(Collectors.toList());
    }
    
    private Document detectLanguageForDocument(Document document) {
        LanguageDetectionResult detection = languageDetector.detectLanguage(document.getContent());
        
        Map<String, Object> metadata = new HashMap<>(document.getMetadata());
        metadata.put("detected_language", detection.getLanguage());
        metadata.put("language_confidence", detection.getConfidence());
        
        return Document.builder()
            .content(document.getContent())
            .metadata(metadata)
            .build();
    }
    
    /**
     * 內容統計增強
     */
    private List<Document> enrichContentStatistics(List<Document> documents) {
        return documents.stream()
            .map(this::calculateContentStatistics)
            .collect(Collectors.toList());
    }
    
    private Document calculateContentStatistics(Document document) {
        String content = document.getContent();
        
        Map<String, Object> metadata = new HashMap<>(document.getMetadata());
        
        // 基本統計
        metadata.put("character_count", content.length());
        metadata.put("word_count", countWords(content));
        metadata.put("sentence_count", countSentences(content));
        metadata.put("paragraph_count", countParagraphs(content));
        metadata.put("estimated_tokens", estimateTokenCount(content));
        
        // 內容特徵
        metadata.put("has_code_blocks", containsCodeBlocks(content));
        metadata.put("has_tables", containsTables(content));
        metadata.put("has_urls", containsUrls(content));
        metadata.put("has_emails", containsEmails(content));
        
        return Document.builder()
            .content(document.getContent())
            .metadata(metadata)
            .build();
    }
    
    /**
     * 自定義分類增強
     */
    private List<Document> enrichCustomClassification(List<Document> documents, MetadataEnrichmentConfig config) {
        return documents.stream()
            .map(doc -> classifyDocument(doc, config.getCustomClassifiers()))
            .collect(Collectors.toList());
    }
    
    private Document classifyDocument(Document document, List<DocumentClassifier> classifiers) {
        Map<String, Object> metadata = new HashMap<>(document.getMetadata());
        
        for (DocumentClassifier classifier : classifiers) {
            ClassificationResult result = classifier.classify(document.getContent());
            metadata.put(classifier.getMetadataKey(), result.getCategory());
            metadata.put(classifier.getMetadataKey() + "_confidence", result.getConfidence());
        }
        
        return Document.builder()
            .content(document.getContent())
            .metadata(metadata)
            .build();
    }
    
    // 輔助方法
    private String calculateContentHash(String content) {
        return DigestUtils.md5Hex(content);
    }
    
    private int countWords(String text) {
        return text.trim().isEmpty() ? 0 : text.trim().split("\\s+").length;
    }
    
    private int countSentences(String text) {
        return text.split("[.!?]+").length;
    }
    
    private int countParagraphs(String text) {
        return text.split("\\n\\s*\\n").length;
    }
    
    private int estimateTokenCount(String text) {
        return (int) Math.ceil(text.length() * 0.25);
    }
    
    private boolean containsCodeBlocks(String text) {
        return text.contains("```") || text.contains("    ") && text.contains("\\n");
    }
    
    private boolean containsTables(String text) {
        return text.contains("|") && text.contains("\\n");
    }
    
    private boolean containsUrls(String text) {
        return text.matches(".*https?://.*");
    }
    
    private boolean containsEmails(String text) {
        return text.matches(".*[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}.*");
    }
}
```

---

## 7.5.4 完整的 ETL 管道實現

### ETL 管道服務

```java
/**
 * 完整的 ETL 管道服務
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class EtlPipelineService {
    
    private final DocumentChunkingService chunkingService;
    private final MetadataEnrichmentService metadataEnrichmentService;
    private final VectorStore vectorStore;
    private final MeterRegistry meterRegistry;
    
    /**
     * 執行完整的 ETL 管道
     */
    public EtlPipelineResult executeEtlPipeline(EtlPipelineConfig config) {
        Timer.Sample sample = Timer.start(meterRegistry);
        EtlPipelineResult result = new EtlPipelineResult();
        
        try {
            // 1. Extract - 提取文檔
            List<Document> extractedDocuments = extractDocuments(config.getDataSources());
            result.setExtractedCount(extractedDocuments.size());
            log.info("Extracted {} documents", extractedDocuments.size());
            
            // 2. Transform - 轉換處理
            List<Document> transformedDocuments = transformDocuments(extractedDocuments, config);
            result.setTransformedCount(transformedDocuments.size());
            log.info("Transformed into {} documents", transformedDocuments.size());
            
            // 3. Load - 載入向量資料庫
            loadDocuments(transformedDocuments, config.getLoadConfig());
            result.setLoadedCount(transformedDocuments.size());
            log.info("Loaded {} documents to vector store", transformedDocuments.size());
            
            result.setSuccess(true);
            result.setProcessingTime(sample.stop(Timer.builder("etl.pipeline.time")
                .description("ETL pipeline execution time")
                .register(meterRegistry)));
            
            meterRegistry.counter("etl.pipeline.success").increment();
            
        } catch (Exception e) {
            log.error("ETL pipeline execution failed", e);
            result.setSuccess(false);
            result.setErrorMessage(e.getMessage());
            meterRegistry.counter("etl.pipeline.errors").increment();
            throw new EtlPipelineException("ETL pipeline execution failed", e);
        }
        
        return result;
    }
    
    /**
     * 提取文檔（Extract 階段）
     */
    private List<Document> extractDocuments(List<DataSource> dataSources) {
        List<Document> allDocuments = new ArrayList<>();
        
        for (DataSource dataSource : dataSources) {
            DocumentReader reader = createDocumentReader(dataSource);
            List<Document> documents = reader.read();
            
            // 添加數據源元資料
            documents = documents.stream()
                .map(doc -> addDataSourceMetadata(doc, dataSource))
                .collect(Collectors.toList());
            
            allDocuments.addAll(documents);
        }
        
        return allDocuments;
    }
    
    /**
     * 轉換文檔（Transform 階段）
     */
    private List<Document> transformDocuments(List<Document> documents, EtlPipelineConfig config) {
        List<Document> transformedDocuments = documents;
        
        // 1. 元資料增強
        if (config.getMetadataEnrichmentConfig() != null) {
            transformedDocuments = metadataEnrichmentService.enrichMetadata(
                transformedDocuments, 
                config.getMetadataEnrichmentConfig()
            );
        }
        
        // 2. 文檔分塊
        if (config.getChunkingConfig() != null) {
            transformedDocuments = chunkingService.chunkDocuments(
                transformedDocuments, 
                config.getChunkingConfig()
            );
        }
        
        // 3. 內容格式化
        if (config.getContentFormatConfig() != null) {
            ContentFormatTransformer formatter = new ContentFormatTransformer(
                config.getContentFormatConfig().getMetadataMode()
            );
            transformedDocuments = formatter.apply(transformedDocuments);
        }
        
        // 4. 過濾和驗證
        transformedDocuments = filterAndValidateDocuments(transformedDocuments, config.getFilterConfig());
        
        return transformedDocuments;
    }
    
    /**
     * 載入文檔（Load 階段）
     */
    private void loadDocuments(List<Document> documents, LoadConfig loadConfig) {
        if (loadConfig.getBatchSize() <= 1) {
            // 單個載入
            vectorStore.accept(documents);
        } else {
            // 批次載入
            List<List<Document>> batches = partitionList(documents, loadConfig.getBatchSize());
            
            for (List<Document> batch : batches) {
                try {
                    vectorStore.accept(batch);
                    
                    if (loadConfig.getBatchDelayMs() > 0) {
                        Thread.sleep(loadConfig.getBatchDelayMs());
                    }
                    
                } catch (Exception e) {
                    log.error("Failed to load batch of {} documents", batch.size(), e);
                    if (!loadConfig.isContinueOnError()) {
                        throw e;
                    }
                }
            }
        }
    }
    
    // 輔助方法
    private DocumentReader createDocumentReader(DataSource dataSource) {
        return switch (dataSource.getType()) {
            case PDF -> new PagePdfDocumentReader(dataSource.getResource());
            case TEXT -> new TextReader(dataSource.getResource());
            case JSON -> new JsonReader(dataSource.getResource());
            case MARKDOWN -> new MarkdownDocumentReader(dataSource.getResource());
            case HTML -> new JsoupDocumentReader(dataSource.getResource());
            default -> throw new IllegalArgumentException("Unsupported data source type: " + dataSource.getType());
        };
    }
    
    private Document addDataSourceMetadata(Document document, DataSource dataSource) {
        Map<String, Object> metadata = new HashMap<>(document.getMetadata());
        metadata.put("data_source_type", dataSource.getType().name());
        metadata.put("data_source_name", dataSource.getName());
        metadata.put("data_source_path", dataSource.getPath());
        
        return Document.builder()
            .content(document.getContent())
            .metadata(metadata)
            .build();
    }
    
    private List<Document> filterAndValidateDocuments(List<Document> documents, FilterConfig filterConfig) {
        if (filterConfig == null) {
            return documents;
        }
        
        return documents.stream()
            .filter(doc -> doc.getContent() != null && !doc.getContent().trim().isEmpty())
            .filter(doc -> doc.getContent().length() >= filterConfig.getMinContentLength())
            .filter(doc -> doc.getContent().length() <= filterConfig.getMaxContentLength())
            .collect(Collectors.toList());
    }
    
    private <T> List<List<T>> partitionList(List<T> list, int batchSize) {
        List<List<T>> partitions = new ArrayList<>();
        for (int i = 0; i < list.size(); i += batchSize) {
            partitions.add(list.subList(i, Math.min(i + batchSize, list.size())));
        }
        return partitions;
    }
}
```

---

## 📝 本章重點回顧

1. **智能文本清理**：實現了多語言支援的文本預處理和標準化
2. **Spring AI 分塊策略**：使用官方 TokenTextSplitter 和自定義語義分塊
3. **元資料增強**：整合 KeywordMetadataEnricher 和 SummaryMetadataEnricher
4. **完整 ETL 管道**：建立了符合 Spring AI 標準的 Extract-Transform-Load 流程
5. **品質保證機制**：添加了監控、錯誤處理和批次處理功能

### 技術要點總結

| 技術點 | Spring AI 官方支援 | 實現難度 | 企業價值 |
|--------|-------------------|----------|----------|
| **TokenTextSplitter** | ⭐⭐⭐ | 中 | 標準化分塊 |
| **KeywordMetadataEnricher** | ⭐⭐⭐ | 中 | AI 關鍵詞提取 |
| **SummaryMetadataEnricher** | ⭐⭐⭐ | 中 | AI 摘要生成 |
| **DocumentReader** | ⭐⭐⭐ | 低 | 多格式支援 |
| **VectorStore 整合** | ⭐⭐⭐ | 低 | 無縫載入 |

### 最佳實踐建議

1. **遵循 Spring AI 標準**：使用官方提供的 DocumentTransformer 實現
2. **合理配置分塊**：根據模型 token 限制調整塊大小
3. **豐富元資料**：充分利用 AI 模型進行關鍵詞和摘要提取
4. **批次處理優化**：根據向量資料庫特性調整批次大小
5. **監控與錯誤處理**：建立完善的 ETL 管道監控機制

### 下一步學習方向

在下一章中，我們將深入學習向量資料庫的選擇與配置，包括：
- 不同向量資料庫的比較和選擇
- Spring AI 向量資料庫整合
- 向量檢索優化策略
- 混合檢索實現

---

**參考資料：**
- [Spring AI ETL Pipeline](https://docs.spring.io/spring-ai/reference/api/etl-pipeline.html)
- [Spring AI Document Transformers](https://docs.spring.io/spring-ai/reference/api/etl-pipeline.html#transformers)
- [TokenTextSplitter Documentation](https://docs.spring.io/spring-ai/reference/api/etl-pipeline.html#_tokentextsplitter)
- [Metadata Enrichers](https://docs.spring.io/spring-ai/reference/api/etl-pipeline.html#_keywordmetadataenricher)