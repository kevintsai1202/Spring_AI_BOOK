# 7.5 ETL (ä¸‹) - çµ¦å‘é‡è³‡æ–™åŠ ä¸Š Buff

> **æœ¬ç« é‡é»**ï¼šæ·±å…¥æ¢è¨ ETL æµç¨‹ä¸­çš„è³‡æ–™è½‰æ›ï¼ˆTransformï¼‰æŠ€è¡“ï¼ŒæŒæ¡æ–‡æœ¬æ¸…ç†ã€æ™ºèƒ½åˆ†å¡Šã€å…ƒè³‡æ–™å¢å¼·å’Œå‘é‡å“è³ªå„ªåŒ–ï¼Œè®“ RAG ç³»çµ±çš„çŸ¥è­˜åº«æ›´åŠ å¼·å¤§å’Œç²¾æº–ã€‚

## ğŸ¯ å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬ç« å­¸ç¿’å¾Œï¼Œæ‚¨å°‡èƒ½å¤ ï¼š

- ğŸ¯ **æŒæ¡æ–‡æœ¬æ¸…ç†æŠ€è¡“**ï¼šå¯¦ç¾æ™ºèƒ½çš„æ–‡æœ¬é è™•ç†å’Œæ¨™æº–åŒ–
- ğŸ¯ **å„ªåŒ–æ–‡æœ¬åˆ†å¡Šç­–ç•¥**ï¼šè¨­è¨ˆé©åˆä¸åŒæ–‡æª”é¡å‹çš„åˆ†å¡Šç®—æ³•
- ğŸ¯ **å¢å¼·å…ƒè³‡æ–™ç®¡ç†**ï¼šå»ºç«‹è±å¯Œçš„å…ƒè³‡æ–™æ¨™è¨˜å’Œåˆ†é¡ç³»çµ±
- ğŸ¯ **æå‡å‘é‡å“è³ª**ï¼šå¯¦ç¾å‘é‡å“è³ªè©•ä¼°å’Œå„ªåŒ–æ©Ÿåˆ¶
- ğŸ¯ **å»ºç«‹å®Œæ•´ ETL ç®¡é“**ï¼šæ•´åˆæå–ã€è½‰æ›ã€è¼‰å…¥çš„å®Œæ•´æµç¨‹

---

## 7.5.1 æ™ºèƒ½æ–‡æœ¬æ¸…ç†èˆ‡é è™•ç†

### æ–‡æœ¬æ¸…ç†çš„é‡è¦æ€§

**ç‚ºä»€éº¼éœ€è¦æ–‡æœ¬æ¸…ç†ï¼Ÿ**
- ğŸ“ **æé«˜å‘é‡å“è³ª**ï¼šæ¸…ç†å¾Œçš„æ–‡æœ¬ç”Ÿæˆæ›´æº–ç¢ºçš„å‘é‡è¡¨ç¤º
- ğŸ¯ **å¢å¼·æª¢ç´¢ç²¾åº¦**ï¼šæ¸›å°‘å™ªéŸ³æé«˜ç›¸ä¼¼æ€§æœå°‹çš„æº–ç¢ºæ€§
- ğŸ’¾ **ç¯€çœå­˜å„²ç©ºé–“**ï¼šç§»é™¤å†—é¤˜å…§å®¹æ¸›å°‘å­˜å„²éœ€æ±‚
- âš¡ **æå‡è™•ç†æ•ˆç‡**ï¼šæ¨™æº–åŒ–æ ¼å¼åŠ å¿«è™•ç†é€Ÿåº¦

### æ–‡æœ¬æ¸…ç†æœå‹™å¯¦ç¾

```java
/**
 * æ™ºèƒ½æ–‡æœ¬æ¸…ç†æœå‹™
 */
@Service
@Slf4j
public class TextCleaningService {
    
    private static final Pattern EMAIL_PATTERN = 
        Pattern.compile("[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}");
    
    private static final Pattern URL_PATTERN = 
        Pattern.compile("https?://[\\w\\-._~:/?#\\[\\]@!$&'()*+,;=%]+");
    
    private static final Pattern PHONE_PATTERN = 
        Pattern.compile("\\b\\d{2,4}[-\\s]?\\d{3,4}[-\\s]?\\d{3,4}\\b");
    
    /**
     * ç¶œåˆæ–‡æœ¬æ¸…ç†
     */
    public String cleanText(String rawText, TextCleaningConfig config) {
        if (rawText == null || rawText.trim().isEmpty()) {
            return "";
        }
        
        String cleanedText = rawText;
        
        // 1. åŸºç¤æ¸…ç†
        if (config.isRemoveExtraWhitespace()) {
            cleanedText = removeExtraWhitespace(cleanedText);
        }
        
        // 2. ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        if (config.isRemoveSpecialCharacters()) {
            cleanedText = removeSpecialCharacters(cleanedText);
        }
        
        // 3. æ¨™æº–åŒ–æ›è¡Œç¬¦
        if (config.isNormalizeLineBreaks()) {
            cleanedText = normalizeLineBreaks(cleanedText);
        }
        
        // 4. ç§»é™¤æ•æ„Ÿè³‡è¨Š
        if (config.isRemoveSensitiveInfo()) {
            cleanedText = removeSensitiveInformation(cleanedText);
        }
        
        // 5. èªè¨€ç‰¹å®šæ¸…ç†
        if (config.getLanguage() != null) {
            cleanedText = applyLanguageSpecificCleaning(cleanedText, config.getLanguage());
        }
        
        // 6. è‡ªå®šç¾©æ¸…ç†è¦å‰‡
        if (config.getCustomRules() != null && !config.getCustomRules().isEmpty()) {
            cleanedText = applyCustomRules(cleanedText, config.getCustomRules());
        }
        
        return cleanedText.trim();
    }
    
    /**
     * ç§»é™¤å¤šé¤˜ç©ºç™½å­—ç¬¦
     */
    private String removeExtraWhitespace(String text) {
        return text
            .replaceAll("\\s+", " ")  // å¤šå€‹ç©ºç™½å­—ç¬¦åˆä½µç‚ºä¸€å€‹
            .replaceAll("\\n\\s*\\n", "\\n\\n")  // å¤šå€‹ç©ºè¡Œåˆä½µç‚ºå…©å€‹
            .trim();
    }
    
    /**
     * ç§»é™¤ç‰¹æ®Šå­—ç¬¦
     */
    private String removeSpecialCharacters(String text) {
        // ä¿ç•™åŸºæœ¬æ¨™é»ç¬¦è™Ÿï¼Œç§»é™¤å…¶ä»–ç‰¹æ®Šå­—ç¬¦
        return text.replaceAll("[^\\p{L}\\p{N}\\p{P}\\p{Z}\\r\\n]", "");
    }
    
    /**
     * æ¨™æº–åŒ–æ›è¡Œç¬¦
     */
    private String normalizeLineBreaks(String text) {
        return text
            .replaceAll("\\r\\n", "\\n")  // Windows æ›è¡Œç¬¦
            .replaceAll("\\r", "\\n");    // Mac æ›è¡Œç¬¦
    }
    
    /**
     * ç§»é™¤æ•æ„Ÿè³‡è¨Š
     */
    private String removeSensitiveInformation(String text) {
        String result = text;
        
        // ç§»é™¤é›»å­éƒµä»¶
        result = EMAIL_PATTERN.matcher(result).replaceAll("[EMAIL]");
        
        // ç§»é™¤ URL
        result = URL_PATTERN.matcher(result).replaceAll("[URL]");
        
        // ç§»é™¤é›»è©±è™Ÿç¢¼
        result = PHONE_PATTERN.matcher(result).replaceAll("[PHONE]");
        
        // ç§»é™¤èº«åˆ†è­‰è™Ÿç¢¼ï¼ˆå°ç£æ ¼å¼ï¼‰
        result = result.replaceAll("\\b[A-Z]\\d{9}\\b", "[ID]");
        
        // ç§»é™¤ä¿¡ç”¨å¡è™Ÿç¢¼
        result = result.replaceAll("\\b\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}[-\\s]?\\d{4}\\b", "[CARD]");
        
        return result;
    }
    
    /**
     * èªè¨€ç‰¹å®šæ¸…ç†
     */
    private String applyLanguageSpecificCleaning(String text, String language) {
        switch (language.toLowerCase()) {
            case "zh":
            case "zh-tw":
            case "zh-cn":
                return cleanChineseText(text);
            case "en":
                return cleanEnglishText(text);
            case "ja":
                return cleanJapaneseText(text);
            default:
                return text;
        }
    }
    
    /**
     * ä¸­æ–‡æ–‡æœ¬æ¸…ç†
     */
    private String cleanChineseText(String text) {
        return text
            // çµ±ä¸€ä¸­æ–‡æ¨™é»ç¬¦è™Ÿ
            .replaceAll("ï¼Œ", "ï¼Œ")
            .replaceAll("ã€‚", "ã€‚")
            .replaceAll("ï¼", "ï¼")
            .replaceAll("ï¼Ÿ", "ï¼Ÿ")
            // ç§»é™¤å…¨å½¢ç©ºæ ¼
            .replaceAll("ã€€", " ")
            // çµ±ä¸€å¼•è™Ÿ
            .replaceAll("[""]", "\"")
            .replaceAll("['']", "'");
    }
    
    /**
     * è‹±æ–‡æ–‡æœ¬æ¸…ç†
     */
    private String cleanEnglishText(String text) {
        return text
            // çµ±ä¸€å¼•è™Ÿ
            .replaceAll("[""]", "\"")
            .replaceAll("['']", "'")
            // ç§»é™¤å¤šé¤˜ç©ºæ ¼
            .replaceAll("\\s+", " ");
    }
    
    /**
     * æ—¥æ–‡æ–‡æœ¬æ¸…ç†
     */
    private String cleanJapaneseText(String text) {
        return text
            // çµ±ä¸€æ—¥æ–‡æ¨™é»ç¬¦è™Ÿ
            .replaceAll("ã€", "ã€")
            .replaceAll("ã€‚", "ã€‚")
            // ç§»é™¤å…¨å½¢ç©ºæ ¼
            .replaceAll("ã€€", " ");
    }
    
    /**
     * æ‡‰ç”¨è‡ªå®šç¾©æ¸…ç†è¦å‰‡
     */
    private String applyCustomRules(String text, List<TextCleaningRule> customRules) {
        String result = text;
        for (TextCleaningRule rule : customRules) {
            result = rule.apply(result);
        }
        return result;
    }
}
```

### æ–‡æœ¬æ¸…ç†é…ç½®é¡

```java
/**
 * æ–‡æœ¬æ¸…ç†é…ç½®
 */
@Data
@Builder
@AllArgsConstructor
@NoArgsConstructor
public class TextCleaningConfig {
    
    @Builder.Default
    private boolean removeExtraWhitespace = true;
    
    @Builder.Default
    private boolean removeSpecialCharacters = false;
    
    @Builder.Default
    private boolean normalizeLineBreaks = true;
    
    @Builder.Default
    private boolean removeSensitiveInfo = true;
    
    private String language;
    
    private List<TextCleaningRule> customRules;
}

/**
 * è‡ªå®šç¾©æ¸…ç†è¦å‰‡æ¥å£
 */
@FunctionalInterface
public interface TextCleaningRule {
    String apply(String text);
}
```

---

## 7.5.2 æ™ºèƒ½æ–‡æœ¬åˆ†å¡Šç­–ç•¥

### ç‚ºä»€éº¼éœ€è¦æ™ºèƒ½åˆ†å¡Šï¼Ÿ

**åˆ†å¡Šçš„é‡è¦æ€§**ï¼š
- ğŸ¯ **é©æ‡‰æ¨¡å‹é™åˆ¶**ï¼šç¬¦åˆ LLM çš„ token é™åˆ¶è¦æ±‚
- ğŸ“ **ä¿æŒèªç¾©å®Œæ•´**ï¼šé¿å…åœ¨èªç¾©é‚Šç•Œä¸­é–“åˆ†å‰²
- ğŸ” **æé«˜æª¢ç´¢ç²¾åº¦**ï¼šåˆé©å¤§å°çš„å¡Šæä¾›æ›´ç²¾ç¢ºçš„ç›¸ä¼¼æ€§åŒ¹é…
- âš¡ **å„ªåŒ–è™•ç†æ•ˆç‡**ï¼šå¹³è¡¡å¡Šå¤§å°èˆ‡è™•ç†é€Ÿåº¦

### Spring AI æ¨™æº–æ–‡æœ¬åˆ†å¡Šå¯¦ç¾

```java
/**
 * æ™ºèƒ½æ–‡æœ¬åˆ†å¡Šæœå‹™
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class DocumentChunkingService {
    
    private final TextCleaningService textCleaningService;
    
    /**
     * ä½¿ç”¨ Spring AI TokenTextSplitter é€²è¡Œåˆ†å¡Š
     */
    public List<Document> chunkDocuments(List<Document> documents, ChunkingConfig config) {
        log.info("Chunking {} documents with config: {}", documents.size(), config);
        
        // 1. æ¸…ç†æ–‡æœ¬
        List<Document> cleanedDocuments = documents.stream()
            .map(doc -> cleanDocument(doc, config.getCleaningConfig()))
            .collect(Collectors.toList());
        
        // 2. å‰µå»º TokenTextSplitter
        TokenTextSplitter splitter = createTokenTextSplitter(config);
        
        // 3. åŸ·è¡Œåˆ†å¡Š
        List<Document> chunkedDocuments = splitter.apply(cleanedDocuments);
        
        log.info("Successfully chunked into {} documents", chunkedDocuments.size());
        
        return chunkedDocuments;
    }
    
    /**
     * å‰µå»º TokenTextSplitter å¯¦ä¾‹
     */
    private TokenTextSplitter createTokenTextSplitter(ChunkingConfig config) {
        return new TokenTextSplitter(
            config.getDefaultChunkSize(),
            config.getMinChunkSizeChars(),
            config.getMinChunkLengthToEmbed(),
            config.getMaxNumChunks(),
            config.isKeepSeparator()
        );
    }
    
    /**
     * æ¸…ç†å–®å€‹æ–‡æª”
     */
    private Document cleanDocument(Document document, TextCleaningConfig cleaningConfig) {
        if (cleaningConfig == null) {
            return document;
        }
        
        String cleanedContent = textCleaningService.cleanText(
            document.getContent(), 
            cleaningConfig
        );
        
        // å‰µå»ºæ–°çš„æ–‡æª”ï¼Œä¿ç•™åŸæœ‰å…ƒè³‡æ–™
        return Document.builder()
            .content(cleanedContent)
            .metadata(new HashMap<>(document.getMetadata()))
            .build();
    }
    
    /**
     * è‡ªå®šç¾©èªç¾©åˆ†å¡Šï¼ˆåŸºæ–¼æ®µè½å’Œå¥å­ï¼‰
     */
    public List<Document> semanticChunking(List<Document> documents, SemanticChunkingConfig config) {
        List<Document> result = new ArrayList<>();
        
        for (Document document : documents) {
            List<Document> documentChunks = semanticChunkDocument(document, config);
            result.addAll(documentChunks);
        }
        
        return result;
    }
    
    /**
     * å°å–®å€‹æ–‡æª”é€²è¡Œèªç¾©åˆ†å¡Š
     */
    private List<Document> semanticChunkDocument(Document document, SemanticChunkingConfig config) {
        String content = document.getContent();
        List<Document> chunks = new ArrayList<>();
        
        // 1. å…ˆæŒ‰æ®µè½åˆ†å‰²
        String[] paragraphs = content.split("\\n\\s*\\n");
        
        StringBuilder currentChunk = new StringBuilder();
        int currentTokenCount = 0;
        int chunkIndex = 0;
        
        for (String paragraph : paragraphs) {
            paragraph = paragraph.trim();
            if (paragraph.isEmpty()) {
                continue;
            }
            
            // ä¼°ç®— token æ•¸é‡ï¼ˆç°¡åŒ–è¨ˆç®—ï¼‰
            int paragraphTokens = estimateTokenCount(paragraph);
            
            // å¦‚æœåŠ å…¥æ­¤æ®µè½æœƒè¶…éé™åˆ¶ï¼Œå…ˆå‰µå»ºä¸€å€‹å¡Š
            if (currentTokenCount + paragraphTokens > config.getMaxTokensPerChunk() 
                && currentChunk.length() > 0) {
                
                Document chunk = createChunk(
                    document, 
                    currentChunk.toString(), 
                    chunkIndex++, 
                    config
                );
                chunks.add(chunk);
                
                currentChunk = new StringBuilder();
                currentTokenCount = 0;
            }
            
            // åŠ å…¥æ®µè½
            if (currentChunk.length() > 0) {
                currentChunk.append("\\n\\n");
            }
            currentChunk.append(paragraph);
            currentTokenCount += paragraphTokens;
        }
        
        // è™•ç†æœ€å¾Œä¸€å€‹å¡Š
        if (currentChunk.length() > 0) {
            Document chunk = createChunk(
                document, 
                currentChunk.toString(), 
                chunkIndex, 
                config
            );
            chunks.add(chunk);
        }
        
        return chunks;
    }
    
    /**
     * å‰µå»ºæ–‡æª”å¡Š
     */
    private Document createChunk(Document originalDoc, String content, int chunkIndex, SemanticChunkingConfig config) {
        Map<String, Object> metadata = new HashMap<>(originalDoc.getMetadata());
        
        // æ·»åŠ åˆ†å¡Šå…ƒè³‡æ–™
        metadata.put("chunk_index", chunkIndex);
        metadata.put("chunk_method", "semantic");
        metadata.put("original_document_id", originalDoc.getId());
        metadata.put("chunk_token_count", estimateTokenCount(content));
        
        // å¦‚æœé…ç½®äº†é‡ç–Šï¼Œæ·»åŠ é‡ç–Šè³‡è¨Š
        if (config.getOverlapTokens() > 0) {
            metadata.put("overlap_tokens", config.getOverlapTokens());
        }
        
        return Document.builder()
            .content(content)
            .metadata(metadata)
            .build();
    }
    
    /**
     * ä¼°ç®—æ–‡æœ¬çš„ token æ•¸é‡ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰
     */
    private int estimateTokenCount(String text) {
        // ç°¡åŒ–ä¼°ç®—ï¼šå¹³å‡æ¯å€‹å­—ç¬¦ç´„ 0.25 tokens
        return (int) Math.ceil(text.length() * 0.25);
    }
}
```

### åˆ†å¡Šé…ç½®é¡

```java
/**
 * åˆ†å¡Šé…ç½®
 */
@Data
@Builder
@AllArgsConstructor
@NoArgsConstructor
public class ChunkingConfig {
    
    /**
     * é»˜èªå¡Šå¤§å°ï¼ˆtokensï¼‰
     */
    @Builder.Default
    private int defaultChunkSize = 1000;
    
    /**
     * æœ€å°å¡Šå¤§å°ï¼ˆå­—ç¬¦ï¼‰
     */
    @Builder.Default
    private int minChunkSizeChars = 350;
    
    /**
     * æœ€å°å¡Šé•·åº¦ä»¥é€²è¡ŒåµŒå…¥
     */
    @Builder.Default
    private int minChunkLengthToEmbed = 10;
    
    /**
     * æœ€å¤§å¡Šæ•¸é‡
     */
    @Builder.Default
    private int maxNumChunks = 10000;
    
    /**
     * æ˜¯å¦ä¿ç•™åˆ†éš”ç¬¦
     */
    @Builder.Default
    private boolean keepSeparator = true;
    
    /**
     * æ–‡æœ¬æ¸…ç†é…ç½®
     */
    private TextCleaningConfig cleaningConfig;
}

/**
 * èªç¾©åˆ†å¡Šé…ç½®
 */
@Data
@Builder
@AllArgsConstructor
@NoArgsConstructor
public class SemanticChunkingConfig {
    
    /**
     * æ¯å€‹å¡Šçš„æœ€å¤§ token æ•¸
     */
    @Builder.Default
    private int maxTokensPerChunk = 1000;
    
    /**
     * å¡Šä¹‹é–“çš„é‡ç–Š token æ•¸
     */
    @Builder.Default
    private int overlapTokens = 100;
    
    /**
     * æ˜¯å¦ä¿æŒæ®µè½å®Œæ•´æ€§
     */
    @Builder.Default
    private boolean preserveParagraphs = true;
    
    /**
     * æ˜¯å¦ä¿æŒå¥å­å®Œæ•´æ€§
     */
    @Builder.Default
    private boolean preserveSentences = true;
}
```

---

## 7.5.3 å…ƒè³‡æ–™å¢å¼·èˆ‡ç®¡ç†

### å…ƒè³‡æ–™çš„é‡è¦æ€§

**ç‚ºä»€éº¼éœ€è¦è±å¯Œçš„å…ƒè³‡æ–™ï¼Ÿ**
- ğŸ¯ **æé«˜æª¢ç´¢ç²¾åº¦**ï¼šåŸºæ–¼å…ƒè³‡æ–™éæ¿¾å’Œæ’åº
- ğŸ“Š **æ”¯æ´åˆ†æçµ±è¨ˆ**ï¼šè¿½è¹¤æ–‡æª”ä¾†æºã€é¡å‹ã€è™•ç†æ™‚é–“
- ğŸ” **å¢å¼·æœç´¢åŠŸèƒ½**ï¼šå¤šç¶­åº¦æœç´¢å’Œéæ¿¾
- ğŸ“ **å¯©è¨ˆè¿½è¹¤**ï¼šè¨˜éŒ„æ•¸æ“šè™•ç†æ­·ç¨‹

### å…ƒè³‡æ–™å¢å¼·æœå‹™

```java
/**
 * å…ƒè³‡æ–™å¢å¼·æœå‹™
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class MetadataEnrichmentService {
    
    private final ChatModel chatModel;
    private final LanguageDetectionService languageDetector;
    
    /**
     * ä½¿ç”¨ Spring AI KeywordMetadataEnricher å¢å¼·é—œéµè©
     */
    public List<Document> enrichWithKeywords(List<Document> documents, int keywordCount) {
        KeywordMetadataEnricher enricher = new KeywordMetadataEnricher(chatModel, keywordCount);
        return enricher.apply(documents);
    }
    
    /**
     * ä½¿ç”¨ Spring AI SummaryMetadataEnricher å¢å¼·æ‘˜è¦
     */
    public List<Document> enrichWithSummaries(List<Document> documents, List<SummaryType> summaryTypes) {
        SummaryMetadataEnricher enricher = new SummaryMetadataEnricher(chatModel, summaryTypes);
        return enricher.apply(documents);
    }
    
    /**
     * ç¶œåˆå…ƒè³‡æ–™å¢å¼·
     */
    public List<Document> enrichMetadata(List<Document> documents, MetadataEnrichmentConfig config) {
        List<Document> enrichedDocuments = new ArrayList<>(documents);
        
        // 1. åŸºç¤å…ƒè³‡æ–™å¢å¼·
        if (config.isEnableBasicMetadata()) {
            enrichedDocuments = enrichBasicMetadata(enrichedDocuments);
        }
        
        // 2. èªè¨€æª¢æ¸¬
        if (config.isEnableLanguageDetection()) {
            enrichedDocuments = enrichLanguageMetadata(enrichedDocuments);
        }
        
        // 3. å…§å®¹çµ±è¨ˆ
        if (config.isEnableContentStatistics()) {
            enrichedDocuments = enrichContentStatistics(enrichedDocuments);
        }
        
        // 4. é—œéµè©æå–ï¼ˆä½¿ç”¨ AIï¼‰
        if (config.isEnableKeywordExtraction() && config.getKeywordCount() > 0) {
            enrichedDocuments = enrichWithKeywords(enrichedDocuments, config.getKeywordCount());
        }
        
        // 5. æ‘˜è¦ç”Ÿæˆï¼ˆä½¿ç”¨ AIï¼‰
        if (config.isEnableSummaryGeneration() && config.getSummaryTypes() != null) {
            enrichedDocuments = enrichWithSummaries(enrichedDocuments, config.getSummaryTypes());
        }
        
        // 6. è‡ªå®šç¾©åˆ†é¡
        if (config.isEnableCustomClassification()) {
            enrichedDocuments = enrichCustomClassification(enrichedDocuments, config);
        }
        
        return enrichedDocuments;
    }
    
    /**
     * åŸºç¤å…ƒè³‡æ–™å¢å¼·
     */
    private List<Document> enrichBasicMetadata(List<Document> documents) {
        return documents.stream()
            .map(this::enrichBasicMetadataForDocument)
            .collect(Collectors.toList());
    }
    
    private Document enrichBasicMetadataForDocument(Document document) {
        Map<String, Object> metadata = new HashMap<>(document.getMetadata());
        
        // è™•ç†æ™‚é–“æˆ³
        metadata.put("processed_at", LocalDateTime.now().toString());
        
        // å…§å®¹å“ˆå¸Œï¼ˆç”¨æ–¼å»é‡ï¼‰
        metadata.put("content_hash", calculateContentHash(document.getContent()));
        
        // æ–‡æª” IDï¼ˆå¦‚æœæ²’æœ‰ï¼‰
        if (!metadata.containsKey("id")) {
            metadata.put("id", UUID.randomUUID().toString());
        }
        
        return Document.builder()
            .content(document.getContent())
            .metadata(metadata)
            .build();
    }
    
    /**
     * èªè¨€æª¢æ¸¬å¢å¼·
     */
    private List<Document> enrichLanguageMetadata(List<Document> documents) {
        return documents.stream()
            .map(this::detectLanguageForDocument)
            .collect(Collectors.toList());
    }
    
    private Document detectLanguageForDocument(Document document) {
        LanguageDetectionResult detection = languageDetector.detectLanguage(document.getContent());
        
        Map<String, Object> metadata = new HashMap<>(document.getMetadata());
        metadata.put("detected_language", detection.getLanguage());
        metadata.put("language_confidence", detection.getConfidence());
        
        return Document.builder()
            .content(document.getContent())
            .metadata(metadata)
            .build();
    }
    
    /**
     * å…§å®¹çµ±è¨ˆå¢å¼·
     */
    private List<Document> enrichContentStatistics(List<Document> documents) {
        return documents.stream()
            .map(this::calculateContentStatistics)
            .collect(Collectors.toList());
    }
    
    private Document calculateContentStatistics(Document document) {
        String content = document.getContent();
        
        Map<String, Object> metadata = new HashMap<>(document.getMetadata());
        
        // åŸºæœ¬çµ±è¨ˆ
        metadata.put("character_count", content.length());
        metadata.put("word_count", countWords(content));
        metadata.put("sentence_count", countSentences(content));
        metadata.put("paragraph_count", countParagraphs(content));
        metadata.put("estimated_tokens", estimateTokenCount(content));
        
        // å…§å®¹ç‰¹å¾µ
        metadata.put("has_code_blocks", containsCodeBlocks(content));
        metadata.put("has_tables", containsTables(content));
        metadata.put("has_urls", containsUrls(content));
        metadata.put("has_emails", containsEmails(content));
        
        return Document.builder()
            .content(document.getContent())
            .metadata(metadata)
            .build();
    }
    
    /**
     * è‡ªå®šç¾©åˆ†é¡å¢å¼·
     */
    private List<Document> enrichCustomClassification(List<Document> documents, MetadataEnrichmentConfig config) {
        return documents.stream()
            .map(doc -> classifyDocument(doc, config.getCustomClassifiers()))
            .collect(Collectors.toList());
    }
    
    private Document classifyDocument(Document document, List<DocumentClassifier> classifiers) {
        Map<String, Object> metadata = new HashMap<>(document.getMetadata());
        
        for (DocumentClassifier classifier : classifiers) {
            ClassificationResult result = classifier.classify(document.getContent());
            metadata.put(classifier.getMetadataKey(), result.getCategory());
            metadata.put(classifier.getMetadataKey() + "_confidence", result.getConfidence());
        }
        
        return Document.builder()
            .content(document.getContent())
            .metadata(metadata)
            .build();
    }
    
    // è¼”åŠ©æ–¹æ³•
    private String calculateContentHash(String content) {
        return DigestUtils.md5Hex(content);
    }
    
    private int countWords(String text) {
        return text.trim().isEmpty() ? 0 : text.trim().split("\\s+").length;
    }
    
    private int countSentences(String text) {
        return text.split("[.!?]+").length;
    }
    
    private int countParagraphs(String text) {
        return text.split("\\n\\s*\\n").length;
    }
    
    private int estimateTokenCount(String text) {
        return (int) Math.ceil(text.length() * 0.25);
    }
    
    private boolean containsCodeBlocks(String text) {
        return text.contains("```") || text.contains("    ") && text.contains("\\n");
    }
    
    private boolean containsTables(String text) {
        return text.contains("|") && text.contains("\\n");
    }
    
    private boolean containsUrls(String text) {
        return text.matches(".*https?://.*");
    }
    
    private boolean containsEmails(String text) {
        return text.matches(".*[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}.*");
    }
}
```

---

## 7.5.4 å®Œæ•´çš„ ETL ç®¡é“å¯¦ç¾

### ETL ç®¡é“æœå‹™

```java
/**
 * å®Œæ•´çš„ ETL ç®¡é“æœå‹™
 */
@Service
@RequiredArgsConstructor
@Slf4j
public class EtlPipelineService {
    
    private final DocumentChunkingService chunkingService;
    private final MetadataEnrichmentService metadataEnrichmentService;
    private final VectorStore vectorStore;
    private final MeterRegistry meterRegistry;
    
    /**
     * åŸ·è¡Œå®Œæ•´çš„ ETL ç®¡é“
     */
    public EtlPipelineResult executeEtlPipeline(EtlPipelineConfig config) {
        Timer.Sample sample = Timer.start(meterRegistry);
        EtlPipelineResult result = new EtlPipelineResult();
        
        try {
            // 1. Extract - æå–æ–‡æª”
            List<Document> extractedDocuments = extractDocuments(config.getDataSources());
            result.setExtractedCount(extractedDocuments.size());
            log.info("Extracted {} documents", extractedDocuments.size());
            
            // 2. Transform - è½‰æ›è™•ç†
            List<Document> transformedDocuments = transformDocuments(extractedDocuments, config);
            result.setTransformedCount(transformedDocuments.size());
            log.info("Transformed into {} documents", transformedDocuments.size());
            
            // 3. Load - è¼‰å…¥å‘é‡è³‡æ–™åº«
            loadDocuments(transformedDocuments, config.getLoadConfig());
            result.setLoadedCount(transformedDocuments.size());
            log.info("Loaded {} documents to vector store", transformedDocuments.size());
            
            result.setSuccess(true);
            result.setProcessingTime(sample.stop(Timer.builder("etl.pipeline.time")
                .description("ETL pipeline execution time")
                .register(meterRegistry)));
            
            meterRegistry.counter("etl.pipeline.success").increment();
            
        } catch (Exception e) {
            log.error("ETL pipeline execution failed", e);
            result.setSuccess(false);
            result.setErrorMessage(e.getMessage());
            meterRegistry.counter("etl.pipeline.errors").increment();
            throw new EtlPipelineException("ETL pipeline execution failed", e);
        }
        
        return result;
    }
    
    /**
     * æå–æ–‡æª”ï¼ˆExtract éšæ®µï¼‰
     */
    private List<Document> extractDocuments(List<DataSource> dataSources) {
        List<Document> allDocuments = new ArrayList<>();
        
        for (DataSource dataSource : dataSources) {
            DocumentReader reader = createDocumentReader(dataSource);
            List<Document> documents = reader.read();
            
            // æ·»åŠ æ•¸æ“šæºå…ƒè³‡æ–™
            documents = documents.stream()
                .map(doc -> addDataSourceMetadata(doc, dataSource))
                .collect(Collectors.toList());
            
            allDocuments.addAll(documents);
        }
        
        return allDocuments;
    }
    
    /**
     * è½‰æ›æ–‡æª”ï¼ˆTransform éšæ®µï¼‰
     */
    private List<Document> transformDocuments(List<Document> documents, EtlPipelineConfig config) {
        List<Document> transformedDocuments = documents;
        
        // 1. å…ƒè³‡æ–™å¢å¼·
        if (config.getMetadataEnrichmentConfig() != null) {
            transformedDocuments = metadataEnrichmentService.enrichMetadata(
                transformedDocuments, 
                config.getMetadataEnrichmentConfig()
            );
        }
        
        // 2. æ–‡æª”åˆ†å¡Š
        if (config.getChunkingConfig() != null) {
            transformedDocuments = chunkingService.chunkDocuments(
                transformedDocuments, 
                config.getChunkingConfig()
            );
        }
        
        // 3. å…§å®¹æ ¼å¼åŒ–
        if (config.getContentFormatConfig() != null) {
            ContentFormatTransformer formatter = new ContentFormatTransformer(
                config.getContentFormatConfig().getMetadataMode()
            );
            transformedDocuments = formatter.apply(transformedDocuments);
        }
        
        // 4. éæ¿¾å’Œé©—è­‰
        transformedDocuments = filterAndValidateDocuments(transformedDocuments, config.getFilterConfig());
        
        return transformedDocuments;
    }
    
    /**
     * è¼‰å…¥æ–‡æª”ï¼ˆLoad éšæ®µï¼‰
     */
    private void loadDocuments(List<Document> documents, LoadConfig loadConfig) {
        if (loadConfig.getBatchSize() <= 1) {
            // å–®å€‹è¼‰å…¥
            vectorStore.accept(documents);
        } else {
            // æ‰¹æ¬¡è¼‰å…¥
            List<List<Document>> batches = partitionList(documents, loadConfig.getBatchSize());
            
            for (List<Document> batch : batches) {
                try {
                    vectorStore.accept(batch);
                    
                    if (loadConfig.getBatchDelayMs() > 0) {
                        Thread.sleep(loadConfig.getBatchDelayMs());
                    }
                    
                } catch (Exception e) {
                    log.error("Failed to load batch of {} documents", batch.size(), e);
                    if (!loadConfig.isContinueOnError()) {
                        throw e;
                    }
                }
            }
        }
    }
    
    // è¼”åŠ©æ–¹æ³•
    private DocumentReader createDocumentReader(DataSource dataSource) {
        return switch (dataSource.getType()) {
            case PDF -> new PagePdfDocumentReader(dataSource.getResource());
            case TEXT -> new TextReader(dataSource.getResource());
            case JSON -> new JsonReader(dataSource.getResource());
            case MARKDOWN -> new MarkdownDocumentReader(dataSource.getResource());
            case HTML -> new JsoupDocumentReader(dataSource.getResource());
            default -> throw new IllegalArgumentException("Unsupported data source type: " + dataSource.getType());
        };
    }
    
    private Document addDataSourceMetadata(Document document, DataSource dataSource) {
        Map<String, Object> metadata = new HashMap<>(document.getMetadata());
        metadata.put("data_source_type", dataSource.getType().name());
        metadata.put("data_source_name", dataSource.getName());
        metadata.put("data_source_path", dataSource.getPath());
        
        return Document.builder()
            .content(document.getContent())
            .metadata(metadata)
            .build();
    }
    
    private List<Document> filterAndValidateDocuments(List<Document> documents, FilterConfig filterConfig) {
        if (filterConfig == null) {
            return documents;
        }
        
        return documents.stream()
            .filter(doc -> doc.getContent() != null && !doc.getContent().trim().isEmpty())
            .filter(doc -> doc.getContent().length() >= filterConfig.getMinContentLength())
            .filter(doc -> doc.getContent().length() <= filterConfig.getMaxContentLength())
            .collect(Collectors.toList());
    }
    
    private <T> List<List<T>> partitionList(List<T> list, int batchSize) {
        List<List<T>> partitions = new ArrayList<>();
        for (int i = 0; i < list.size(); i += batchSize) {
            partitions.add(list.subList(i, Math.min(i + batchSize, list.size())));
        }
        return partitions;
    }
}
```

---

## ğŸ“ æœ¬ç« é‡é»å›é¡§

1. **æ™ºèƒ½æ–‡æœ¬æ¸…ç†**ï¼šå¯¦ç¾äº†å¤šèªè¨€æ”¯æ´çš„æ–‡æœ¬é è™•ç†å’Œæ¨™æº–åŒ–
2. **Spring AI åˆ†å¡Šç­–ç•¥**ï¼šä½¿ç”¨å®˜æ–¹ TokenTextSplitter å’Œè‡ªå®šç¾©èªç¾©åˆ†å¡Š
3. **å…ƒè³‡æ–™å¢å¼·**ï¼šæ•´åˆ KeywordMetadataEnricher å’Œ SummaryMetadataEnricher
4. **å®Œæ•´ ETL ç®¡é“**ï¼šå»ºç«‹äº†ç¬¦åˆ Spring AI æ¨™æº–çš„ Extract-Transform-Load æµç¨‹
5. **å“è³ªä¿è­‰æ©Ÿåˆ¶**ï¼šæ·»åŠ äº†ç›£æ§ã€éŒ¯èª¤è™•ç†å’Œæ‰¹æ¬¡è™•ç†åŠŸèƒ½

### æŠ€è¡“è¦é»ç¸½çµ

| æŠ€è¡“é» | Spring AI å®˜æ–¹æ”¯æ´ | å¯¦ç¾é›£åº¦ | ä¼æ¥­åƒ¹å€¼ |
|--------|-------------------|----------|----------|
| **TokenTextSplitter** | â­â­â­ | ä¸­ | æ¨™æº–åŒ–åˆ†å¡Š |
| **KeywordMetadataEnricher** | â­â­â­ | ä¸­ | AI é—œéµè©æå– |
| **SummaryMetadataEnricher** | â­â­â­ | ä¸­ | AI æ‘˜è¦ç”Ÿæˆ |
| **DocumentReader** | â­â­â­ | ä½ | å¤šæ ¼å¼æ”¯æ´ |
| **VectorStore æ•´åˆ** | â­â­â­ | ä½ | ç„¡ç¸«è¼‰å…¥ |

### æœ€ä½³å¯¦è¸å»ºè­°

1. **éµå¾ª Spring AI æ¨™æº–**ï¼šä½¿ç”¨å®˜æ–¹æä¾›çš„ DocumentTransformer å¯¦ç¾
2. **åˆç†é…ç½®åˆ†å¡Š**ï¼šæ ¹æ“šæ¨¡å‹ token é™åˆ¶èª¿æ•´å¡Šå¤§å°
3. **è±å¯Œå…ƒè³‡æ–™**ï¼šå……åˆ†åˆ©ç”¨ AI æ¨¡å‹é€²è¡Œé—œéµè©å’Œæ‘˜è¦æå–
4. **æ‰¹æ¬¡è™•ç†å„ªåŒ–**ï¼šæ ¹æ“šå‘é‡è³‡æ–™åº«ç‰¹æ€§èª¿æ•´æ‰¹æ¬¡å¤§å°
5. **ç›£æ§èˆ‡éŒ¯èª¤è™•ç†**ï¼šå»ºç«‹å®Œå–„çš„ ETL ç®¡é“ç›£æ§æ©Ÿåˆ¶

### ä¸‹ä¸€æ­¥å­¸ç¿’æ–¹å‘

åœ¨ä¸‹ä¸€ç« ä¸­ï¼Œæˆ‘å€‘å°‡æ·±å…¥å­¸ç¿’å‘é‡è³‡æ–™åº«çš„é¸æ“‡èˆ‡é…ç½®ï¼ŒåŒ…æ‹¬ï¼š
- ä¸åŒå‘é‡è³‡æ–™åº«çš„æ¯”è¼ƒå’Œé¸æ“‡
- Spring AI å‘é‡è³‡æ–™åº«æ•´åˆ
- å‘é‡æª¢ç´¢å„ªåŒ–ç­–ç•¥
- æ··åˆæª¢ç´¢å¯¦ç¾

---

**åƒè€ƒè³‡æ–™ï¼š**
- [Spring AI ETL Pipeline](https://docs.spring.io/spring-ai/reference/api/etl-pipeline.html)
- [Spring AI Document Transformers](https://docs.spring.io/spring-ai/reference/api/etl-pipeline.html#transformers)
- [TokenTextSplitter Documentation](https://docs.spring.io/spring-ai/reference/api/etl-pipeline.html#_tokentextsplitter)
- [Metadata Enrichers](https://docs.spring.io/spring-ai/reference/api/etl-pipeline.html#_keywordmetadataenricher)